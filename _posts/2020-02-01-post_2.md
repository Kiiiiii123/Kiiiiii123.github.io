### CS285 Lecture1: Introduction and Course Overview
### (1) What we'll cover?
#### 1. From supervised learning to decision making
#### 2. Model-free algorithms: Q-learning, policy gradients, actor-critic
#### 3. Advanced model learning and prediction
#### 4. Transfer and multi-task learning, meta-learning
#### 5. Exploration
#### 6. Open problems, research talks, invited lectures
### (2) Assignments
#### 1. Homework 1: Imitation learning (control via supervised learning)
#### 2. Homework 2: Policy gradients (“REINFORCE”)
#### 3. Homework 3: Q learning and actor-critic algorithms
#### 4. Homework 4: Model-based reinforcement learning
#### 5. Homework 5: Advanced model-free RL algorithms
#### 6. Final project: Research-level project of your choice
### (3) Deep learning helps us handle unstructured environments,Reinforcement learning provides a formalism for behavior
![](/images/1.png)
### (4) End-to-end learning means a lot for sequential decision making 
![](/images/2.png)
### (5) Deep models are what allow reinforcement learning algorithms to solve complex problems end to end!
### (6) Why should we study this now?
#### 1. Advances in deep learning
#### 2. Advances in reinforcement learning
#### 3. Advances in computational capability
### (7) Beyond learning from reward - What other problems do we need to solve to enable real-world sequential decision making?
#### 1. Basic reinforcement learning deals with maximizing rewards
#### 2. This is not the only problem that matters for sequential decision making!
#### 3. We will cover more advanced topics
##### • Learning reward functions from example (inverse reinforcement learning)
##### • Transferring knowledge between domains (transfer learning, meta-learning)
##### • Learning to predict and using prediction to act
