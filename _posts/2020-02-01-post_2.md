### CS285 Lecture1: Introduction and Course Overview
### (1) What we'll cover?
#### 1. From supervised learning to decision making
#### 2. Model-free algorithms: Q-learning, policy gradients, actor-critic
#### 3. Advanced model learning and prediction
#### 4. Transfer and multi-task learning, meta-learning
#### 5. Exploration
#### 6. Open problems, research talks, invited lectures
### (2) Assignments
#### 1. Homework 1: Imitation learning (control via supervised learning)
#### 2. Homework 2: Policy gradients (“REINFORCE”)
#### 3. Homework 3: Q learning and actor-critic algorithms
#### 4. Homework 4: Model-based reinforcement learning
#### 5. Homework 5: Advanced model-free RL algorithms
#### 6. Final project: Research-level project of your choice
### (3) Deep learning helps us handle unstructured environments,Reinforcement learning provides a formalism for behavior
### (4) End-to-end learning means a lot for sequential decision making 
![](/images/2.png)
### (5) Deep models are what allow reinforcement learning algorithms to solve complex problems end to end!
### (6) Why should we study this now?
#### 1. Advances in deep learning
#### 2. Advances in reinforcement learning
#### 3. Advances in computational capability
### (7) Beyond learning from reward - What other problems do we need to solve to enable real-world sequential decision making?
#### 1. Basic reinforcement learning deals with maximizing rewards
#### 2. This is not the only problem that matters for sequential decision making!
#### 3. We will cover more advanced topics
##### • Learning reward functions from example (inverse reinforcement learning)
##### • Transferring knowledge between domains (transfer learning, meta-learning)
##### • Learning to predict and using prediction to act
### (8) Are there other forms of supervision?
#### 1. Learning from demonstrations
##### • Directly copying observed behavior
##### • Inferring rewards from observed behavior (inverse reinforcement learning)
#### 2. Learning from observing the world
##### • Learning to predict
##### • unsupervised learning
#### 3. Learning from other tasks
##### • Transfer learning
##### • Meta-learning: learning to learn
### (9) Learning as the basis of intelligence
#### 1. Some things we can all do (e.g. walking)
#### 2. Some things we can only learn (e.g. driving a car)
#### 3. We can learn a huge variety of things, including very difficult things
#### 4. Therefore our learning mechanism(s) are likely powerful enough to do everything we associate with intelligence
##### • But it may still be very convenient to “hard-code” a few really important bits
### (10) What must that single algorithm do?
#### 1. Interpret rich sensory inputs
#### 2. Choose complex actions
### (11) Why deep reinforcement learning?
#### 1. Deep = can process complex sensory input
##### • …and also compute really complex functions
#### 2. Reinforcement learning = can choose complex actions
### (12) What can deep learning & RL do well now?
#### 1. Acquire high degree of proficiency in domains governed by simple, known rules
#### 2. Learn simple skills with raw sensory inputs, given enough experience
#### 3. Learn from imitating enough humanprovided expert behavior
### (13) What has proven challenging so far?
#### 1. Humans can learn incredibly quickly - Deep RL methods are usually slow
#### 2. Humans can reuse past knowledge - Transfer learning in deep RL is an open problem
#### 3. Not clear what the reward function should be
#### 4. Not clear what the role of prediction should be
**Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's? If this were then subjected to an appropriate course of education one would obtain the adult brain. - Alan Turing**

