### CS285 Lecture2: Supervised Learning of Behaviors
### (1) Today’s Lecture
#### 1. Definition of sequential decision problems
#### 2. Imitation learning: supervised learning for decision making
##### • Does direct imitation work?
##### • How can we make it work more often?
#### 3. A little bit of theory
#### 4. Case studies of recent work in (deep) imitation learning
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand basic imitation learning algorithms
##### • Understand tools for theoretical analysis
### (2) Terminology & notation
<p align="center">
<img src="/images/4.png"><br/>
</p>

### (3) Graphical model
#### 1. Observation is produced from the state
#### 2. Transition function: How does your current state in your current action influence the next state
#### 3. Markov property for sequential decision making: You're trying to choose a2 and you already know s2 then nothing about s1 or any proceding state will help you make a better decision, because s2 is all you need to predict everything  
#### 4. Partially observed o1 may be helpful for o2, but s2 is enough
#### 5. Some algorithms are designed for fully observed problems but still use them for partially observed problems
#### 6. Imitation learning really works with the partially observed setting, Q-learning works with fully observed problems
<p align="center">
<img src="/images/5.png"><br/>
</p>

### (4) Imitation learning
#### 1. Behavioral cloning - regular supervised learning
<p align="center">
<img src="/images/6.png"><br/>
</p>

#### 2. Does it work? No, there're some reasons
##### • Might see something new
##### • People make mistakes
##### • Small mistakes at first, large mistakes at the end
<p align="center">
<img src="/images/7.png"><br/>
</p>

#### 3. Does it work? Yes, three cameras to teach the network how to correct the mistakes
<p align="center">
<img src="/images/8.png"><br/>
Nvidia Research
</p>

#### 4. Can we make it more often?
##### • Generate some distribution with a lot of noise - We can have some way to generate not just the demonstration that cause the training process to drift. We can have a distribution over trajectories, a distribution where the user's behaviors perturb by some kind of noise forcing them to correct that noise. If we introduce the noise, the expert will be forced to correct the noise and we'll get the full distribution. Then when our policy drifts a little bit from the optimal behavior, the dataset already has some samples with similar mistakes, that will teach the network how to correct itself.
<p align="center">
<img src="/images/9.png"><br/>
</p>

#### 5. Can we construct an algorithm for imitation learning that can guarante the limits to solve at least under idealize conditions?
##### • When your policy samples from the training set, it actually samples from P-data
##### • When we run PI theta, we actually observe observations from a different distribution, 
