### CS285 Lecture2: Supervised Learning of Behaviors
### (1) Today’s Lecture
#### 1. Definition of sequential decision problems
#### 2. Imitation learning: supervised learning for decision making
##### • Does direct imitation work?
##### • How can we make it work more often?
#### 3. A little bit of theory
#### 4. Case studies of recent work in (deep) imitation learning
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand basic imitation learning algorithms
##### • Understand tools for theoretical analysis
### (2) Terminology & notation
![](/images/4.png)
### (3) Graphical model
#### 1. Observation is produced from the state
#### 2. Transition function: How does your current state in your current action influence the next state
#### 3. Markov property for sequential decision making: You're trying to choose a2 and you already know s2 then nothing about s1 or any proceding state will help you make a better decision, because s2 is all you need to predict everything  
#### 4. Partially observed o1 may be helpful for o2, but s2 is enough
#### 5. Some algorithms are designed for fully observed problems but still use them for partially observed problems
#### 6. Imitation learning really works with the partially observed setting, Q-learning works with fully observed problems
![](/images/5.png)
### (4) Imitation learning
#### 1. Behavioral cloning - regular supervised learning
![](/images/6.png)
#### 2. Does it work? No, there're some reasons
##### • Might see something new
##### • People make mistakes
##### • Small mistakes at first, large mistakes at the end
![](/images/7.png)
#### 3. Does it work? Yes, three cameras to teach the network how to correct the mistakes
![](/images/8.png)
#### 4. Can we make it more often?
![](/images/9.png)


