### CS285 Lecture2: Supervised Learning of Behaviors
### (1) Today’s Lecture
#### 1. Definition of sequential decision problems
#### 2. Imitation learning: supervised learning for decision making
##### • Does direct imitation work?
##### • How can we make it work more often?
#### 3. A little bit of theory
#### 4. Case studies of recent work in (deep) imitation learning
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand basic imitation learning algorithms
##### • Understand tools for theoretical analysis
### (2) Terminology & notation
<p align="center">
<img src="/images/4.png"><br/>
</p>

### (3) Graphical model
#### 1. Observation is produced from the state
#### 2. Transition function: How does your current state in your current action influence the next state
#### 3. Markov property for sequential decision making: You're trying to choose a2 and you already know s2 then nothing about s1 or any proceding state will help you make a better decision, because s2 is all you need to predict everything  
#### 4. Partially observed o1 may be helpful for o2, but s2 is enough
#### 5. Some algorithms are designed for fully observed problems but still use them for partially observed problems
#### 6. Imitation learning really works with the partially observed setting, Q-learning works with fully observed problems
<p align="center">
<img src="/images/5.png"><br/>
</p>

### (4) Imitation learning
#### 1. Behavioral cloning - regular supervised learning
<p align="center">
<img src="/images/6.png"><br/>
</p>

#### 2. Does it work? No, there're some reasons
##### • Might see something new
##### • People make mistakes
##### • Small mistakes at first, large mistakes at the end
<p align="center">
<img src="/images/7.png"><br/>
</p>

#### 3. Does it work? Yes, three cameras to teach the network how to correct the mistakes
<p align="center">
<img src="/images/8.png"><br/>
Nvidia Research
</p>

#### 4. Can we make it more often?
##### • Generate some distribution with a lot of noise - We can have some way to generate not just the demonstration that cause the training process to drift. We can have a distribution over trajectories, a distribution where the user's behaviors perturb by some kind of noise forcing them to correct that noise. If we introduce the noise, the expert will be forced to correct the noise and we'll get the full distribution. Then when our policy drifts a little bit from the optimal behavior, the dataset already has some samples with similar mistakes, that will teach the network how to correct itself.
<p align="center">
<img src="/images/9.png"><br/>
</p>

#### 5. Can we construct an algorithm for imitation learning that can guarante the limits to solve at least under idealize conditions?
##### • When your policy samples from the training set, it actually samples from P-data
##### • When we run PI theta, we actually observe observations from a different distribution, that is P-PI-theta, which is not in general equal to P-data
<p align="center">
<img src="/images/10.png"><br/>
</p>

#### 6. One of the ways to fix upper problem is to devise an algorithm that will actually cause P-data to be equal to P-PI-theta. Instead of changing P-PI-theta to be perfect, we can change P-data so that the distribution shift goes away. This is the idea behind  the *DAgger: Dataset Aggregation* algorithm. If you run for long time, the distributional dismatch will be eliminated
<p align="center">
<img src="/images/11.png"><br/>
DAgger: Dataset Aggregation
</p>

##### • What's the problem?
###### Most of the issues with upper algorithm really have to do with the third step with asking the human for the additional labels
##### • Can we make it work without more data? (human work)
###### DAgger addresses the problem of distributional “drift”
###### What if our model is so good that it doesn’t drift?
###### Need to mimic expert behavior very accurately
###### But don’t overfit!
##### • Why might we fail to fit the expert?
###### Non-Markovian behavior - People may already be committed to the plan and choose to act in non-markovian way
###### Multimodal behavior - People may choose to do different things in similar situations
##### • To solve Non-Markovian behavior - How can we use the whole history?
###### We don't want to feed all the frames into a standard feed-forward convolutional neural network, because we have a huge number of channels here so we might have too many weights. The number of previous frames might vary
###### We can try to design a recurrent architecture and there's many ways to design it.
<p align="center">
<img src="/images/12.png"><br/>
The simplest design
</p>

##### • To solve Multimodal behavior - How can we handle multiple modes?
###### Multimodal behavior is actually no problem if you have discrete actions
###### If you have continuous actions, the most common way to train policies that output continuous actions is to use a Gaussian distribution, using a mean squared error loss(the log probability of a Gaussian). If the output is either the mean or the mean and variance of a Gaussian, this is a unimodal distribution. If you try to fit the data, you will get exactly the thing that you don't want
###### The first choice is to output a mixture of Gaussians, with N means, N variences and a scalar weight on each of these entries. The scalar wasting to sum up to one and we need to a softmax function. The network is sometimes reffered as mixture density networks. We need to pick the number of mixture elements. In practice, if we have one or two dimensions of actions, this design tends to work very well. But if we have extremely high dimensional actions, the choice of N might end up being very delicate.
<p align="center">
<img src="/images/13.png"><br/>
</p>

###### The second choice is to use latent variable, it's more complicated to set up but it will not change the output - a unimodal Gaussian. We'll actually inject an additional input into the network at the bottom, which is some kind of random number sampled from a Gaussian distribution or a uniform distribution. The network will be trained using the randomness to actually change the output distribution. We need to look up some of the techniques that will teach the network to use the noise effectively, like conditional variational autoencoder, Normalizing flow/realNVP, stein variational gradient descent. Just inject noise into the network is not enough because the network doesn't have any reason to use the noise and the noise doesn't correlate with the output.
<p align="center">
<img src="/images/14.png"><br/>
</p>

###### The third choice is to use autoregressive discretization. It's a lot simpler, but it requires quite a few changes to the network. With discrete actions, there's no problem because we can use a softmax function and represent complicated multimodal distributions. With conntinuous actions, if we can discretize actions well, that's perfectly fine. But if you discretize high-dimensional continuous variables then you're going to get the curse of dimensionality, you are going to get so many discredization bins which will run out of memory. Autoregressive discretization deals with this problem by discretizing one dimension at a time.
<p align="center">
<img src="/images/15.png"><br/>
</p>

#### 7. Imitation learning: recap
##### • Often (but not always) insufficient by itself
###### Distribution mismatch problem
##### • Sometimes works well 
###### Hacks (e.g. left/right images)
###### Samples from a stable trajectory distribution
###### Add more on-policy data, e.g. using Dagger
###### Better models that fit more accurately
#### 8. Other topics in imitation learning
##### • Structured prediction problems like machine translation
##### • Inverse reinforcement learning
###### Use demonstrations to figure out what somebody's goal is and then use regular reinforcement learning to figure out how to achieve that goal.
#### 8. Imitation learning: what's the problem?
##### • Humans need to provide data, which is typically finite
###### Deep learning works best when data is plentiful
##### • Humans are not good at providing some kinds of actions
###### Low level stuff like how fast throw to some cells should spin, which voltage to apply to the motors. Maybe a robot have 20 legs...
##### • Humans can learn autonomously; can our machines do the same?
###### Unlimited data from own experience
###### Continuous self-improvement











