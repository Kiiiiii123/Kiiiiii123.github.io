### CS285 Lecture2: Supervised Learning of Behaviors
### (1) Today’s Lecture
#### 1. Definition of sequential decision problems
#### 2. Imitation learning: supervised learning for decision making
##### • Does direct imitation work?
##### • How can we make it work more often?
#### 3. A little bit of theory
#### 4. Case studies of recent work in (deep) imitation learning
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand basic imitation learning algorithms
##### • Understand tools for theoretical analysis
### (2) Terminology & notation
<p align="center">
<img src="/images/4.png"><br/>
</p>

### (3) Graphical model
#### 1. Observation is produced from the state
#### 2. Transition function: How does your current state in your current action influence the next state
#### 3. Markov property for sequential decision making: You're trying to choose a2 and you already know s2 then nothing about s1 or any proceding state will help you make a better decision, because s2 is all you need to predict everything  
#### 4. Partially observed o1 may be helpful for o2, but s2 is enough
#### 5. Some algorithms are designed for fully observed problems but still use them for partially observed problems
#### 6. Imitation learning really works with the partially observed setting, Q-learning works with fully observed problems
<p align="center">
<img src="/images/5.png"><br/>
</p>

### (4) Imitation learning
#### 1. Behavioral cloning - regular supervised learning
<p align="center">
<img src="/images/6.png"><br/>
</p>

#### 2. Does it work? No, there're some reasons
##### • Might see something new
##### • People make mistakes
##### • Small mistakes at first, large mistakes at the end
<p align="center">
<img src="/images/7.png"><br/>
</p>

#### 3. Does it work? Yes, three cameras to teach the network how to correct the mistakes
<p align="center">
<img src="/images/8.png"><br/>
Nvidia Research
</p>

#### 4. Can we make it more often?
##### • Generate some distribution with a lot of noise - We can have some way to generate not just the demonstration that cause the training process to drift. We can have a distribution over trajectories, a distribution where the user's behaviors perturb by some kind of noise forcing them to correct that noise. If we introduce the noise, the expert will be forced to correct the noise and we'll get the full distribution. Then when our policy drifts a little bit from the optimal behavior, the dataset already has some samples with similar mistakes, that will teach the network how to correct itself.
<p align="center">
<img src="/images/9.png"><br/>
</p>

#### 5. Can we construct an algorithm for imitation learning that can guarante the limits to solve at least under idealize conditions?
##### • When your policy samples from the training set, it actually samples from P-data
##### • When we run PI theta, we actually observe observations from a different distribution, that is P-PI-theta, which is not in general equal to P-data
<p align="center">
<img src="/images/10.png"><br/>
</p>

#### 6. One of the ways to fix upper problem is to devise an algorithm that will actually cause P-data to be equal to P-PI-theta. Instead of changing P-PI-theta to be perfect, we can change P-data so that the distribution shift goes away. This is the idea behind  the *DAgger: Dataset Aggregation* algorithm. If you run for long time, the distributional dismatch will be eliminated
<p align="center">
<img src="/images/11.png"><br/>
DAgger: Dataset Aggregation
</p>

##### • What's the problem?
###### Most of the issues with upper algorithm really have to do with the third step with asking the human for the additional labels
##### • Can we make it work without more data? (human work)
###### DAgger addresses the problem of distributional “drift”
###### What if our model is so good that it doesn’t drift?
###### Need to mimic expert behavior very accurately
###### But don’t overfit!
##### • Why might we fail to fit the expert?
###### Non-Markovian behavior - People may already be committed to the plan and choose to act in non-markovian way
###### Multimodal behavior - People may choose to do different things in similar situations
##### • To solve Non-Markovian behavior - How can we use the whole history?
###### We don't want to feed all the frames into a standard feed-forward convolutional neural network, because we have a huge number of channels here so we might have too many weights. The number of previous frames might vary
###### We can try to design a recurrent architecture and there's many ways to design it.
<p align="center">
<img src="/images/12.png"><br/>
The simplest design
</p>

##### • To solve Multimodal behavior - How can we handle multiple modes?
###### Multimodal behavior is actually no problem if you have discrete actions
###### If you have continuous actions, the most common way to train policies that output continuous actions is to use a Gaussian distribution, using a mean squared error loss(the log probability of a Gaussian). If the output is either the mean or the mean and variance of a Gaussian, this is a unimodal distribution. If you try to fit the data, you will get exactly the thing that you don't want
###### The first choice is to output a mixture of Gaussians, with N means, N variences and a scalar weight on each of these entries. The scalar wasting to sum up to one and we need to a softmax function. The network is sometimes reffered as mixture density networks. We need to pick the number of mixture elements. In practice, if we have one or two dimensions of actions, this design tends to work very well. But if we have extremely high dimensional actions, the choice of N might end up being very delicate.
<p align="center">
<img src="/images/13.png"><br/>
</p>

###### The second choice is to use latent variable, it's more complicated to set up but it will not change the output - a unimodal Gaussian. We'll actually inject an additional input into the network at the bottom, which is some kind of random number sampled from a Gaussian distribution or a uniform distribution. The network will be trained using the randomness to actually change the output distribution. We need to look up some of the techniques that will teach the network to use the noise effectively, like conditional variational autoencoder, Normalizing flow/realNVP, stein variational gradient descent. Just inject noise into the network is not enough because the network doesn't have any reason to use the noise and the noise doesn't correlate with the output.
<p align="center">
<img src="/images/14.png"><br/>
</p>

###### The third choice is to use autoregressive discretization. It's a lot simpler, but it requires quite a few changes to the network. With discrete actions, there's no problem because we can use a softmax function and represent complicated multimodal distributions. With conntinuous actions, if we can discretize actions well, that's perfectly fine. But if you discretize high-dimensional continuous variables then you're going to get the curse of dimensionality, you are going to get so many discredization bins which will run out of memory. Autoregressive discretization deals with this problem by discretizing one dimension at a time.
<p align="center">
<img src="/images/15.png"><br/>
</p>

#### 7. Imitation learning: recap
##### • Often (but not always) insufficient by itself
###### Distribution mismatch problem
##### • Sometimes works well 
###### Hacks (e.g. left/right images)
###### Samples from a stable trajectory distribution
###### Add more on-policy data, e.g. using Dagger
###### Better models that fit more accurately
#### 8. Other topics in imitation learning
##### • Structured prediction problems like machine translation
##### • Inverse reinforcement learning
###### Use demonstrations to figure out what somebody's goal is and then use regular reinforcement learning to figure out how to achieve that goal.
#### 8. Imitation learning: what's the problem?
##### • Humans need to provide data, which is typically finite
###### Deep learning works best when data is plentiful
##### • Humans are not good at providing some kinds of actions
###### Low level stuff like how fast throw to some cells should spin, which voltage to apply to the motors. Maybe a robot have 20 legs...
##### • Humans can learn autonomously; can our machines do the same?
###### Unlimited data from own experience
###### Continuous self-improvement
#### 9. A cost function for imitation learning?
<p align="center">
<img src="/images/16.png"><br/>
</p>

##### • Some analysis
###### We have DAgger algorithm to solve the distribution mismatch problem, but now we want to know how bad is it? Can we somehow quantify our bound how bad it is?
###### We are going to use capital T to denote the length of our trajectory (how long we are going to act for). It's a finite number. 
<p align="center">
<img src="/images/17.png"><br/>
</p>

###### We analyse the case of zero-one loss
<p align="center">
<img src="/images/18.png"><br/>
</p>

###### Things are gonna be pretty bad if you do direct imitation. We assume that the algoritnm is not completely broken. We assume that the probability of not matching the expert is less than or equal to epsilon for any state you saw during training.You didn't screw up in setting up your learning algorithm on the states you trained it. It's gonna have a small probability epsilon of making a mistake but on states you didn't see all bets are off.
<p align="center">
<img src="/images/19.png"><br/>
</p>

###### What we care about is the expected value of the total cost (the sum of the costs over all the time steps from 1 to capital T). Even if the epsilon is small, for a long enough trajectory, you are gonna pay a heavy price.
<p align="center">
<img src="/images/20.png"><br/>
</p>

##### • More general analysis
###### We don't want to make assumptions like this because you're bounded by epsilon literally in the states you saw in the training set. That's a very weak assumpytion. When provided with enough training data, supervised learning algorithms generalize to other samples from the same distribution. So we can make a stronger assumption that the error will be bounded by epsilon for any state sampled from the training distribution even if it was not identical to one of the training states. The assumption is important because you're not going to see the image twice, but you might see images that are from the same distribution that are similar in various ways.
<p align="center">
<img src="/images/21.png"><br/>
</p>

###### A very short discussion of what happens with DAgger (what's DAgger's contribution to the cost)
<p align="center">
<img src="/images/22.png"><br/>
</p>

###### When P-train is not equal to P-data, we'll get a much worse bound (P means distribution). The following identity allows us to relate one distribution to another. Distribution of states at a particular time step t, the same distribution as the expert (the probability of not making a mistake after t time steps), some other distribution totally different from the expert (the probability that you've made at least one mistake). 
<p align="center">
<img src="/images/23.png"><br/>
</p>

###### Then we can relate distributions to each other in terms of what's called total variation divergence. It's a funny way of saying take the absolute value of the difference between two probabilities and some of those absolute values up over all the states.
<p align="center">
<img src="/images/24.png"><br/>
</p>

###### Then we can analyze the quantity that taht we actually care about which is the expected cost. It's a sum over the time steps of the expected cost. We can see that in the more general case where we are dealing with probability distributions, the bound is still epsilon T squared. That's basically showing that in the general case, if we don't match the distributions, we might get still quadratic cost.
<p align="center">
<img src="/images/25.png"><br/>
</p>

##### • A few notes on cost/reward functions
###### So far we talked about this kind of nice thoery, we have simple costs, everything is very easy to analyze, but things are gonna get a bit more complicated in regard to reward functions especially in practice. You might think that you can get away with very simple reard functions (like 0-1), but in practice we often have much more complex cost/reward functions. Part of the reason why we often end up with fairly complex reward functions is that these very simple reward functions pose a very large exploration problem and it's hard for reinforcement learning algorithms to discover successful strategies even once so they can begin improving.
<p align="center">
<img src="/images/26.png"><br/>
</p>
