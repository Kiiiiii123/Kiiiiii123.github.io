### CS285 Lecture4: Introduction to Reinforcement Learning
### (1) Today’s Lecture
#### 1. Definition of a Markov decision process
#### 2. Definition of reinforcement learning problem
#### 3. Anatomy of a RL algorithm
#### 4. Brief overview of RL algorithm types
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand the underlying reinforcement learning objective
##### • Get summary of possible algorithms
### (2) Definitions
#### 1. s, a, r(s,a), p(s'|s,a) define Markov decision process - states, actions, rewards, transitions
#### 2. Markov chain
<p align="center">
<img src="/images/31.png"><br/>
</p>

#### 3. Markov decision process (MDP) - introduce some actions and rewards
<p align="center">
<img src="/images/32.png"><br/>
</p>

#### 4. Partially observed Markov decision process (POMDP) - introduce observation. It's a generalization of Markov decision processes. It's the most general form of the reinforcement learning problem.
<p align="center">
<img src="/images/33.png"><br/>
</p>

### (3) The goal of reinforcement learning
<p align="center">
<img src="/images/34.png"><br/>
</p>

#### 1. We can write down an equation for the probability of seeing a sequence of states and actions (trajectory) just via the chain rule. It constructs the trajectory distribution. Our objective is to find the parameter vector θ* to maximize the expectation of total reward under the trajectory distribution. We have to write it as an expectation because in general the states and actions you'll see for a particular parameter vector are random variables. Now in many reinforcement learning problems, we might not even know what p(st+1|st,at) is, but we can approximate it in various ways.
<p align="center">
<img src="/images/35.png"><br/>
</p>

#### 2. Break down the subjective a little more and equivalently turn the right side of the equation to a Markov chain on (s,a).
<p align="center">
<img src="/images/36.png"><br/>
</p>

<p align="center">
<img src="/images/37.png"><br/>
</p>


















