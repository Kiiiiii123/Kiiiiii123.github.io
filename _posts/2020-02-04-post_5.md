### CS285 Lecture4: Introduction to Reinforcement Learning
### (1) Today’s Lecture
#### 1. Definition of a Markov decision process
#### 2. Definition of reinforcement learning problem
#### 3. Anatomy of a RL algorithm
#### 4. Brief overview of RL algorithm types
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand the underlying reinforcement learning objective
##### • Get summary of possible algorithms
### (2) Definitions
#### 1. s, a, r(s,a), p(s'|s,a) define Markov decision process - states, actions, rewards, transitions
#### 2. Markov chain
<p align="center">
<img src="/images/31.png"><br/>
</p>

#### 3. Markov decision process (MDP) - introduce some actions and rewards
<p align="center">
<img src="/images/32.png"><br/>
</p>

#### 4. Partially observed Markov decision process (POMDP) - introduce observation. It's a generalization of Markov decision processes. It's the most general form of the reinforcement learning problem.
<p align="center">
<img src="/images/33.png"><br/>
</p>

### (3) The goal of reinforcement learning
<p align="center">
<img src="/images/34.png"><br/>
</p>

#### 1. We can write down an equation for the probability of seeing a sequence of states and actions (trajectory) just via the chain rule. It constructs the trajectory distribution. Our objective is to find the parameter vector θ* to maximize the expectation of total reward under the trajectory distribution. We have to write it as an expectation because in general the states and actions you'll see for a particular parameter vector are random variables. Now in many reinforcement learning problems, we might not even know what p(st+1|st,at) is, but we can approximate it in various ways.
<p align="center">
<img src="/images/35.png"><br/>
</p>

#### 2. Break down the subjective a little more and equivalently turn the right side of the equation to a Markov chain on (s,a).
<p align="center">
<img src="/images/36.png"><br/>
</p>

<p align="center">
<img src="/images/37.png"><br/>
</p>

#### 3. Finite horizon case: state-action marginal
##### We can do a few things with the upper objective. For example, we can write it out as a sum of the expectations over state-action marginals.
<p align="center">
<img src="/images/38.png"><br/>
</p>

#### 4. Infinite horizon case: stationary distribution
##### • Infinite means capital T is infinite. Redefining things in the state-action marginal way is not actually like all that useful right now, but one of the things that lets us do which is very useful is derive an objective for reinforcement learning in the infinite horizon case. It seems a little perplexing because the sum over an infinite number of terms. The state-action translation operator is obtained by multiplying together the MDP transition operator by the policy. Stationary distributions means that once you take your vector of probabilities and you hit it with T, you should get back the same vector of probabilities because that means your distribution is not changing any more. That doesn't mean your state is not changing, so you might be moving around to different states but on average your distribution remains the same. We can recognize this as an eigenvector problem, μ is the eigenvector of T with eigenvalue 1, it always does under some mild regularity conditions and one condition that you need is a reversibility which means that you can always come back to any place that you came from. If you have dead ends in your MDP, or if your MDP sort of split into two halves where you can never go from one half to another, this might not hold. But in general if you can come back to any place where you started, then you're going to have this property that the stationary distribution exists which means that you'll have an eigenvector with eigenvalue 1.
<p align="center">
<img src="/images/39.png"><br/>
</p>

##### • Now we can an infinite horizon objective. We don't want to sum over infinite so we put a one over T into the objective otherwise the sum of rewards will also be infinite. This will give you a finite number reffered as the average reward objective.
<p align="center">
<img src="/images/40.png"><br/>
</p>

#### 5. Expectations and stochastic systems
##### • Both of the objectives whether you're inthe infinite horizon case or in the finite horizon case are the expectation of some functions. What you're optimizing is a vector of parameters that affect the distribution under which the expectation is the biggest.
<p align="center">
<img src="/images/41.png"><br/>
</p>

##### • In RL, we almost always care about expectations. The point is that we may just have objectives reward functions that are non-smooth and discontinuous, it doesn't mean that the resulting reinforcement learning problem is not smooth. Reinforcement learning always cares about expectations, and expectations of non-smooth discontinuous functions under nice well-behaved probability distributions can actually be smooth. So this is why we can do gradient-based optimization in RL even when we have really complicated non-differentiable reward functions.
<p align="center">
<img src="/images/42.png"><br/>
</p>

### (4) The anatomy of a reinforcement learning algorithm
#### 1. Every algorithm that will be covered in this course can be broken down into three parts.
<p align="center">
<img src="/images/43.png"><br/>
</p>

##### • A simple example
<p align="center">
<img src="/images/44.png"><br/>
</p>

##### • Another example: RL by backprop - model-based approach: learn a function to predict the next state
<p align="center">
<img src="/images/45.png"><br/>
</p>

#### • Which parts are expensive?
<p align="center">
<img src="/images/46.png"><br/>
</p>

### (5) How do we deal with all these expectations?
#### 1. We can express the expectation in a convenient kind of recursive way. We write it as a set of nested expectations. Because of the Markov property, the number of these expectations is pretty big but each one is only conditioned on one or two variables.
<p align="center">
<img src="/images/47.png"><br/>
</p>

<p align="center">
<img src="/images/48.png"><br/>
</p>

#### 2. The function of s1 and a1 Q(s1,a1) is equal to the inner part of the expectation. If we know what that is, then the reinforcement learning objective will actually simplify drastically. The point is that if you knew this quantity (Q function value),you could make your policy better without having to worry too much about long horizon consequences.
<p align="center">
<img src="/images/49.png"><br/>
</p>

#### 3. Definition: Q-function
##### Q-PI means Q-function corresponding to the policy PI. Q function estimates all these expectations and all these expectations depend on a particular policy. It's just the expectation of the rewards for all future steps. It's the total reward you'll get from current time step untill the end in expectation under your transition dynamics and your policy. In RL algorithms we'll see various ways to approximate this quantity. Evaluating this quantity exactly is typically intractable for all but small MDPs but there're many ways to approximate it at a high level. You can estimate expectations using samples. Samples are typically used to estimate Q function and there're many ways to sample. 
<p align="center">
<img src="/images/50.png"><br/>
</p>

#### 4. Definition: value function
##### It's very similar with Q-function. The value function is just a version of Q-function that does not take actions as input. The value function also depends on a policy. It's a function of only the state. It's just the sum of the expected rewards given the state.
<p align="center">
<img src="/images/51.png"><br/>
</p>

#### 5. Use Q-functions and value functions
##### High level intuition about why these things are actually useful.
<p align="center">
<img src="/images/52.png"><br/>
</p>


