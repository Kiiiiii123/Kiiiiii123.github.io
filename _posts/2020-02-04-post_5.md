### CS285 Lecture4: Introduction to Reinforcement Learning
### (1) Today’s Lecture
#### 1. Definition of a Markov decision process
#### 2. Definition of reinforcement learning problem
#### 3. Anatomy of a RL algorithm
#### 4. Brief overview of RL algorithm types
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand the underlying reinforcement learning objective
##### • Get summary of possible algorithms
### (2) Definitions
#### 1. s, a, r(s,a), p(s'|s,a) define Markov decision process - states, actions, rewards, transitions
#### 2. Markov chain
<p align="center">
<img src="/images/31.png"><br/>
</p>

#### 3. Markov decision process (MDP) - introduce some actions and rewards
<p align="center">
<img src="/images/32.png"><br/>
</p>

#### 4. Partially observed Markov decision process (POMDP) - introduce observation. It's a generalization of Markov decision processes. It's the most general form of the reinforcement learning problem.
<p align="center">
<img src="/images/33.png"><br/>
</p>

### (3) The goal of reinforcement learning
<p align="center">
<img src="/images/34.png"><br/>
</p>

#### 1. We can write down an equation for the probability of seeing a sequence of states and actions (trajectory) just via the chain rule. It constructs the trajectory distribution. Our objective is to find the parameter vector θ* to maximize the expectation of total reward under the trajectory distribution. We have to write it as an expectation because in general the states and actions you'll see for a particular parameter vector are random variables. Now in many reinforcement learning problems, we might not even know what p(st+1|st,at) is, but we can approximate it in various ways.
<p align="center">
<img src="/images/35.png"><br/>
</p>

#### 2. Break down the subjective a little more and equivalently turn the right side of the equation to a Markov chain on (s,a).
<p align="center">
<img src="/images/36.png"><br/>
</p>

<p align="center">
<img src="/images/37.png"><br/>
</p>

#### 3. Finite horizon case: state-action marginal
##### We can do a few things with the upper objective. For example, we can write it out as a sum of the expectations over state-action marginals.
<p align="center">
<img src="/images/38.png"><br/>
</p>

#### 4. Infinite horizon case: stationary distribution
##### Infinite means capital T is infinite. Redefining things in the state-action marginal way is not actually like all that useful right now, but one of the things that lets us do which is very useful is derive an objective for reinforcement learning in the infinite horizon case. It seems a little perplexing because the sum over an infinite number of terms. The state-action translation operator is obtained by multiplying together the MDP transition operator by the policy. Stationary distributions means that once you take your vector of probabilities and you hit it with T, you should get back the same vector of probabilities because that means your distribution is not changing any more. That doesn't mean your state is not changing, so you might be moving around to different states but on average your distribution remains the same. We can recognize this as an eigenvector problem, μ is the eigenvector of T with eigenvalue 1, it always does under some mild regularity conditions and one condition that you need is a reversibility which means that you can always come back to any place that you came from. If you have dead ends in your MDP, or if your MDP sort of split into two halves where you can never go from one half to another, this might not hold. But in general if you can come back to any place where you started, then you're going to have this property that the stationary distribution exists which means that you'll have an eigenvector with eigenvalue 1.
<p align="center">
<img src="/images/39.png"><br/>
</p>













