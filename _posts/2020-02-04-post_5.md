### CS285 Lecture4: Introduction to Reinforcement Learning
### (1) Today’s Lecture
#### 1. Definition of a Markov decision process
#### 2. Definition of reinforcement learning problem
#### 3. Anatomy of a RL algorithm
#### 4. Brief overview of RL algorithm types
#### 5. Goals:
##### • Understand definitions & notation
##### • Understand the underlying reinforcement learning objective
##### • Get summary of possible algorithms
### (2) Definitions
#### 1. s, a, r(s,a), p(s'|s,a) define Markov decision process - states, actions, rewards, transitions
#### 2. Markov chain
<p align="center">
<img src="/images/31.png"><br/>
</p>

#### 3. Markov decision process (MDP) - introduce some actions and rewards
<p align="center">
<img src="/images/32.png"><br/>
</p>

#### 4. Partially observed Markov decision process (POMDP) - introduce observation. It's a generalization of Markov decision processes. It's the most general form of the reinforcement learning problem.
<p align="center">
<img src="/images/33.png"><br/>
</p>

### (3) The goal of reinforcement learning
<p align="center">
<img src="/images/34.png"><br/>
</p>

#### 1. We can write down an equation for the probability of seeing a sequence of states and actions (trajectory) just via the chain rule. It constructs the trajectory distribution. Our objective is to find the parameter vector θ* to maximize the expectation of total reward under the trajectory distribution. We have to write it as an expectation because in general the states and actions you'll see for a particular parameter vector are random variables. Now in many reinforcement learning problems, we might not even know what p(st+1|st,at) is, but we can approximate it in various ways.
<p align="center">
<img src="/images/35.png"><br/>
</p>

#### 2. Break down the subjective a little more and equivalently turn the right side of the equation to a Markov chain on (s,a).
<p align="center">
<img src="/images/36.png"><br/>
</p>

<p align="center">
<img src="/images/37.png"><br/>
</p>

#### 3. Finite horizon case: state-action marginal
##### We can do a few things with the upper objective. For example, we can write it out as a sum of the expectations over state-action marginals.
<p align="center">
<img src="/images/38.png"><br/>
</p>

#### 4. Infinite horizon case: stationary distribution
##### • Infinite means capital T is infinite. Redefining things in the state-action marginal way is not actually like all that useful right now, but one of the things that lets us do which is very useful is derive an objective for reinforcement learning in the infinite horizon case. It seems a little perplexing because the sum over an infinite number of terms. The state-action translation operator is obtained by multiplying together the MDP transition operator by the policy. Stationary distributions means that once you take your vector of probabilities and you hit it with T, you should get back the same vector of probabilities because that means your distribution is not changing any more. That doesn't mean your state is not changing, so you might be moving around to different states but on average your distribution remains the same. We can recognize this as an eigenvector problem, μ is the eigenvector of T with eigenvalue 1, it always does under some mild regularity conditions and one condition that you need is a reversibility which means that you can always come back to any place that you came from. If you have dead ends in your MDP, or if your MDP sort of split into two halves where you can never go from one half to another, this might not hold. But in general if you can come back to any place where you started, then you're going to have this property that the stationary distribution exists which means that you'll have an eigenvector with eigenvalue 1.
<p align="center">
<img src="/images/39.png"><br/>
</p>

##### • Now we can an infinite horizon objective. We don't want to sum over infinite so we put a one over T into the objective otherwise the sum of rewards will also be infinite. This will give you a finite number reffered as the average reward objective.
<p align="center">
<img src="/images/40.png"><br/>
</p>

#### 5. Expectations and stochastic systems
##### • Both of the objectives whether you're inthe infinite horizon case or in the finite horizon case are the expectation of some functions. What you're optimizing is a vector of parameters that affect the distribution under which the expectation is the biggest.
<p align="center">
<img src="/images/41.png"><br/>
</p>

##### • In RL, we almost always care about expectations. The point is that we may just have objectives reward functions that are non-smooth and discontinuous, it doesn't mean that the resulting reinforcement learning problem is not smooth. Reinforcement learning always cares about expectations, and expectations of non-smooth discontinuous functions under nice well-behaved probability distributions can actually be smooth. So this is why we can do gradient-based optimization in RL even when we have really complicated non-differentiable reward functions.
<p align="center">
<img src="/images/42.png"><br/>
</p>

### (4) The anatomy of a reinforcement learning algorithm
#### 1. Every algorithm that will be covered in this course can be broken down into three parts.
<p align="center">
<img src="/images/43.png"><br/>
</p>

##### • A simple example
<p align="center">
<img src="/images/44.png"><br/>
</p>

##### • Another example: RL by backprop - model-based approach: learn a function to predict the next state
<p align="center">
<img src="/images/45.png"><br/>
</p>

#### • Which parts are expensive?
<p align="center">
<img src="/images/46.png"><br/>
</p>

### (5) How do we deal with all these expectations?
#### 1. We can express the expectation in a convenient kind of recursive way. We write it as a set of nested expectations. Because of the Markov property, the number of these expectations is pretty big but each one is only conditioned on one or two variables.
<p align="center">
<img src="/images/47.png"><br/>
</p>

<p align="center">
<img src="/images/48.png"><br/>
</p>

#### 2. The function of s1 and a1 Q(s1,a1) is equal to the inner part of the expectation. If we know what that is, then the reinforcement learning objective will actually simplify drastically. The point is that if you knew this quantity (Q function value),you could make your policy better without having to worry too much about long horizon consequences.
<p align="center">
<img src="/images/49.png"><br/>
</p>

#### 3. Definition: Q-function
##### Q-PI means Q-function corresponding to the policy PI. Q function estimates all these expectations and all these expectations depend on a particular policy. It's just the expectation of the rewards for all future steps. It's the total reward you'll get from current time step untill the end in expectation under your transition dynamics and your policy. In RL algorithms we'll see various ways to approximate this quantity. Evaluating this quantity exactly is typically intractable for all but small MDPs but there're many ways to approximate it at a high level. You can estimate expectations using samples. Samples are typically used to estimate Q function and there're many ways to sample. 
<p align="center">
<img src="/images/50.png"><br/>
</p>

#### 4. Definition: value function
##### It's very similar with Q-function. The value function is just a version of Q-function that does not take actions as input. The value function also depends on a policy. It's a function of only the state. It's just the sum of the expected rewards given the state.
<p align="center">
<img src="/images/51.png"><br/>
</p>

#### 5. Use Q-functions and value functions
##### High level intuition about why these things are actually useful.
<p align="center">
<img src="/images/52.png"><br/>
</p>

### (6) Types of RL algorithms
#### 1. The algorithms all try to tackle the objective but they'll do it in different ways. Note: Actor-critic algorithms basically do a combination of policy gradients and value-based algorithms. They improve the policy using something very similar to policy gradients. They calculate a gradient using value-functions and Q-functions.
<p align="center">
<img src="/images/53.png"><br/>
</p>

#### 2. Model-based RL algorithms
<p align="center">
<img src="/images/54.png"><br/>
</p>

##### What are some ways that we can improve a policy if we have learned a model?
<p align="center">
<img src="/images/55.png"><br/>
</p>

#### 3. Value function based algorithms
##### Create some approximation to fit Q-function or value function using samples. We won't actually have an explict policy represented by some other neural network. The new policy is just the argmax of the Q-function with respect to a. For discrete actions this is very easy, but for continuous actions it's a lot involved but still possible.
<p align="center">
<img src="/images/56.png"><br/>
</p>

#### 4. Direct policy gradients
<p align="center">
<img src="/images/57.png"><br/>
</p>

#### 5. Actor-critic: value functions + policy gradients
##### We still need to fit our value function or Q-function and then we get a better estimate for the gradient by using our V or Q.
<p align="center">
<img src="/images/58.png"><br/>
</p>

### (7) Tradeoffs and why there are so many different RL methods when they're solving the same problem?
#### 1. There are so many RL algorithms because we don't quite know how to best solve the problem and it seems like different methods work better in different situations. There are several factors that we'll consider when we choose or design our algorithm.
<p align="center">
<img src="/images/59.png"><br/>
</p>

#### 2. Comparison: sample efficiency
##### • The difference between on-policy algorithms and off-policy is whether the algorithm can use the data that came from other policies.
<p align="center">
<img src="/images/60.png"><br/>
</p>

##### • We have both on-policy and off-policy actor-critic algorithms. Model-based algorithms because they can do a lot of computation for every single batch of samples.
<p align="center">
<img src="/images/61.png"><br/>
</p>

##### • Why should we use a less efficient algorithm?
###### Because the efficiency may be not the only thing you care about, one thing you may care about is computation time.
##### • Wall clock time is not the same as efficiency
###### In fact, it's almost the opposite. If it is very easy for you to simulate experience and most of your computation cost comes from the learning part, this thing will basically be reversed. With enough parallelism, gradient-free methods can be some of the fastest. Some of the model-based RL methods can be some of the most expensive computationally because they have to do a lot of modeling for every sample they generate. They actually should fit the models backpropagating through all those stuff and all costs compute. In terms of compute, if your simulation is cheap, the picture is often the opposite. There are some other considerations, algorithms that are further to the left on this line will have additional assumptions. Fro example, Q-function based methods typically assume full observability so they assume you have the Markovian state not just observation whereas policy gradient methods often don't care about partial observability. You typically get more assumptions as you go to the left. 
#### 3. Comparison: stability and ease of use
##### • Q-learning is a fixed point iteration procedure that actually can not be proven to converge under function approximate. In model-based RL, you're doing gradient-based optimization but on the wrong objective because the model is optimized for better understanding the world. So convergence in practice or in theories are problematic and we have to spend more time fidding with various deep learning tricks to get our algorithms to be more stable.
<p align="center">
<img src="/images/62.png"><br/>
</p>

##### • Value functions don't directly optimize for expected reward when you fit the value function. They minimize the error of the fit meaning the difference between your learned value function and the expected value, which is not the same as actually optimizing your reward you might get. A better fit for your value function doesn't necessarily mean that you get a better policy. At worst, value function doesn't even optimize that, so most of the popular methods for value function fitting can't even be proven to converge to the best error fit under function approximation. Model-based RL, the model minimizes the error of fit and you can get that part to converge. But minimize the error of fit for the model is not the same as getting a better policy too. Policy gradient is the only one that actually performs gradient-based optimization on the true objective but comes with a number of other downsides like requiring on-policy sampling.
<p align="center">
<img src="/images/63.png"><br/>
</p>

#### 4. Comparison: assumptions
##### There are some assumptions that some methods will make. Note: The episodic learning means that your learning process is separated into clearlt delineated trials.
<p align="center">
<img src="/images/64.png"><br/>
</p>

### (8) Examples of specific algorithms
<p align="center">
<img src="/images/65.png"><br/>
</p>















