### CS285 Lecture5: Policy Gradients
### (1) Today’s Lecture
#### 1. The policy gradient algorithm
#### 2. What does the policy gradient do?
#### 3. Basic variance reduction: causality （因果关系）
#### 4. Basic variance reduction: baselines
#### 5. Policy gradient examples
#### 6. Goals:
##### • Understand policy gradient reinforcement learning 
##### • Understand practical considerations for policy gradients
### (2) Evaluating the RL objective
#### J(θ) is equal to the expectation and we can approximate it using samples. We get an unbiased estimate of the expected reward of our policy.
<p align="center">
<img src="/images/66.png"><br/>
</p>

### (3) Direct policy differentiation
#### 1. Pθ(τ) and πθ(τ) mean exactly the same thing. An integral can be written as an expectation.
<p align="center">
<img src="/images/67.png"><br/>
</p>

#### 2. We're trying to get the gradient of the whole thing, so we'll ignore the first quantity. We'll also ignore the third quantity because we can push the gradient inside the sum. Just like before, we evaluated our RL objective value by generating samples bu actually running our policy in the world to get an estimate of an expectation. Now we'll do exactly the same thing. We sum together the rewards and sum together these grad-log-πs, multiply them and then average that.
<p align="center">
<img src="/images/68.png"><br/>
</p>

### (4) Evaluating the objective
#### There's an equation that we can actually implement in a computer to estimate the policy gradient using N samples.
<p align="center">
<img src="/images/69.png"><br/>
</p>

### (5) Comparison to maximum likelihood
#### How the policy gradient algorithms compare to standard supervised learning (maximum likelihood estimation)? They are actually quite similar except that the policy gradient multiply those grad-log-πs by these total rewards. So we can implement it in a very similar way as how we implement maximum likelihood training.
<p align="center">
<img src="/images/70.png"><br/>
</p>

### (6) Example: Gaussian policies
#### This example was for continuous actions. We need πθ(a|s) to define a distribution over some continuous value. A very popular choice is to use a multivariant normal distribution. In a multivariant normal distribution, your policy's mean （均值向量） is defined by some neural networkand its covariance （协方差矩阵）might also be defined by some neural network but for now we'll just say the covariance is constant for simplicity. 
<p align="center">
<img src="/images/71.png"><br/>
</p>





