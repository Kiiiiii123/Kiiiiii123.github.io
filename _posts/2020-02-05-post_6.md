### CS285 Lecture5: Policy Gradients
### (1) Today’s Lecture
#### 1. The policy gradient algorithm
#### 2. What does the policy gradient do?
#### 3. Basic variance reduction: causality （因果关系）
#### 4. Basic variance reduction: baselines
#### 5. Policy gradient examples
#### 6. Goals:
##### • Understand policy gradient reinforcement learning 
##### • Understand practical considerations for policy gradients
### (2) Evaluating the RL objective
#### J(θ) is equal to the expectation and we can approximate it using samples. We get an unbiased estimate of the expected reward of our policy.
<p align="center">
<img src="/images/66.png"><br/>
</p>

### (3) Direct policy differentiation
#### 1. Pθ(τ) and πθ(τ) mean exactly the same thing. An integral can be written as an expectation.
<p align="center">
<img src="/images/67.png"><br/>
</p>

#### 2. We're trying to get the gradient of the whole thing, so we'll ignore the first quantity. We'll also ignore the third quantity because we can push the gradient inside the sum. Just like before, we evaluated our RL objective value by generating samples bu actually running our policy in the world to get an estimate of an expectation. Now we'll do exactly the same thing. We sum together the rewards and sum together these grad-log-πs, multiply them and then average that.
<p align="center">
<img src="/images/68.png"><br/>
</p>

### (4) Evaluating the objective
#### There's an equation that we can actually implement in a computer to estimate the policy gradient using N samples.
<p align="center">
<img src="/images/69.png"><br/>
</p>

### (5) Comparison to maximum likelihood
#### How the policy gradient algorithms compare to standard supervised learning (maximum likelihood estimation)? They are actually quite similar except that the policy gradient multiply those grad-log-πs by these total rewards. So we can implement it in a very similar way as how we implement maximum likelihood training.
<p align="center">
<img src="/images/70.png"><br/>
</p>

### (6) Example: Gaussian policies
#### This example was for continuous actions. We need πθ(a|s) to define a distribution over some continuous value. A very popular choice is to use a multivariant normal distribution. In a multivariant normal distribution, your policy's mean （均值向量） is defined by some neural networkand its covariance （协方差矩阵）might also be defined by some neural network but for now we'll just say the covariance is constant for simplicity. 
<p align="center">
<img src="/images/71.png"><br/>
</p>

### (7) What did we just do?
#### Let's talk about the intuition of what's going on here. How does this algorithm actually change the policy in practice. The gradient of log πθ(τ) basically points in the direction that will increase the probability of the trajectory τ with high reward. However, the maximum likelihood gradient makes everythin more probable.
<p align="center">
<img src="/images/72.png"><br/>
</p>

### (8) Policy gradient has high variance 
#### What's wrong with policy gradient? Let's say that I observed three samples and two of those samples had rewards that were small positive numbers and one sample had a reward that was a very negative number. If we use those three samples to make our policy gradient, we might expect taht our policy will slide over to the right a little bit and it;ll somewhat increase the probability on these two samples and greatly decrease the probability on the negative sample. If we take the same exact reward function and add a big constant to the samples' rewards, the relative rewards are exactly the same, but the policy gradient obtained from these rewards will be very different because now the policy is trying to make this more probable. The trajectory distribution will spread out a little bit to cover all three of samples. This should be weird to us because we haven't actually change the problem. This is one illustrative example to explain how the policy gradient has high variance. When you get different samples, you get very different gradient estimats. For a small finite number of samples, you might get very different policy gradients if you test multiple sample batches. This is a big problem because is means that your gradient is very noisy. If you estimate your gradient with a modest number of samples, you won't go straight to the optimum, you'll take a really zigzag path. For too large learning rate, you might never get the optimum.
<p align="center">
<img src="/images/73.png"><br/>
</p>

### (9) Review
<p align="center">
<img src="/images/74.png"><br/>
</p>

### (10) To reduce high variance
#### 1. Exploiting causality 
##### The past influences the future, but the future doesn't influence the past. Intuitively that the variance is bigger if the quantity is bigger, so if I were to take all the rewards and make them ten times smaller numerically, the variance would also be smaller. We have the grad-log probability of the action at time step t but it's being multiplied by a bunch of rewards that includes rewards that happen before and after time step t. We know that the action that we take at time step t cannot affect the reward at some other step t' that happened prior to t.
<p align="center">tuitively
<img src="/images/75.png"><br/>
</p>

#### 2. Baselines
##### What we would like to do is to take things that are better than average and make those more likely, and take the stuff that is worse than average and make those less likely.
<p align="center">tuitively
<img src="/images/76.png"><br/>
</p>

#### 3. Analyzing variance
##### We analyze the variance to derive optimal baselines. We use g(τ) to denote grad-θ-log-π.
<p align="center">tuitively
<img src="/images/77.png"><br/>
</p>

### (11) Policy gradient is on-policy
















