### CS285 Lecture 5: Policy Gradients
### (1) Today’s Lecture
#### 1. The policy gradient algorithm
#### 2. What does the policy gradient do?
#### 3. Basic variance reduction: causality （因果关系）
#### 4. Basic variance reduction: baselines
#### 5. Policy gradient examples
#### 6. Goals:
##### • Understand policy gradient reinforcement learning 
##### • Understand practical considerations for policy gradients
### (2) Evaluating the RL objective
#### J(θ) is equal to the expectation and we can approximate it using samples. We get an unbiased estimate of the expected reward of our policy.
<p align="center">
<img src="/images/66.png"><br/>
</p>

### (3) Direct policy differentiation
#### 1. Pθ(τ) and πθ(τ) mean exactly the same thing. An integral can be written as an expectation.
<p align="center">
<img src="/images/67.png"><br/>
</p>

#### 2. We're trying to get the gradient of the whole thing, so we'll ignore the first quantity. We'll also ignore the third quantity because we can push the gradient inside the sum. Just like before, we evaluated our RL objective value by generating samples bu actually running our policy in the world to get an estimate of an expectation. Now we'll do exactly the same thing. We sum together the rewards and sum together these grad-log-πs, multiply them and then average that.
<p align="center">
<img src="/images/68.png"><br/>
</p>

### (4) Evaluating the objective
#### There's an equation that we can actually implement in a computer to estimate the policy gradient using N samples.
<p align="center">
<img src="/images/69.png"><br/>
</p>

### (5) Comparison to maximum likelihood
#### How the policy gradient algorithms compare to standard supervised learning (maximum likelihood estimation)? They are actually quite similar except that the policy gradient multiply those grad-log-πs by these total rewards. So we can implement it in a very similar way as how we implement maximum likelihood training.
<p align="center">
<img src="/images/70.png"><br/>
</p>

### (6) Example: Gaussian policies
#### This example was for continuous actions. We need πθ(a|s) to define a distribution over some continuous value. A very popular choice is to use a multivariant normal distribution. In a multivariant normal distribution, your policy's mean （均值向量） is defined by some neural networkand its covariance （协方差矩阵）might also be defined by some neural network but for now we'll just say the covariance is constant for simplicity. 
<p align="center">
<img src="/images/71.png"><br/>
</p>

### (7) What did we just do?
#### Let's talk about the intuition of what's going on here. How does this algorithm actually change the policy in practice. The gradient of log πθ(τ) basically points in the direction that will increase the probability of the trajectory τ with high reward. However, the maximum likelihood gradient makes everythin more probable.
<p align="center">
<img src="/images/72.png"><br/>
</p>

### (8) Policy gradient has high variance 
#### What's wrong with policy gradient? Let's say that I observed three samples and two of those samples had rewards that were small positive numbers and one sample had a reward that was a very negative number. If we use those three samples to make our policy gradient, we might expect taht our policy will slide over to the right a little bit and it;ll somewhat increase the probability on these two samples and greatly decrease the probability on the negative sample. If we take the same exact reward function and add a big constant to the samples' rewards, the relative rewards are exactly the same, but the policy gradient obtained from these rewards will be very different because now the policy is trying to make this more probable. The trajectory distribution will spread out a little bit to cover all three of samples. This should be weird to us because we haven't actually change the problem. This is one illustrative example to explain how the policy gradient has high variance. When you get different samples, you get very different gradient estimats. For a small finite number of samples, you might get very different policy gradients if you test multiple sample batches. This is a big problem because is means that your gradient is very noisy. If you estimate your gradient with a modest number of samples, you won't go straight to the optimum, you'll take a really zigzag path. For too large learning rate, you might never get the optimum.
<p align="center">
<img src="/images/73.png"><br/>
</p>

### (9) Review1
<p align="center">
<img src="/images/74.png"><br/>
</p>

### (10) To reduce high variance
#### 1. Exploiting causality 
##### The past influences the future, but the future doesn't influence the past. Intuitively that the variance is bigger if the quantity is bigger, so if I were to take all the rewards and make them ten times smaller numerically, the variance would also be smaller. We have the grad-log probability of the action at time step t but it's being multiplied by a bunch of rewards that includes rewards that happen before and after time step t. We know that the action that we take at time step t cannot affect the reward at some other step t' that happened prior to t.
<p align="center">tuitively
<img src="/images/75.png"><br/>
</p>

#### 2. Baselines
##### What we would like to do is to take things that are better than average and make those more likely, and take the stuff that is worse than average and make those less likely.
<p align="center">tuitively
<img src="/images/76.png"><br/>
</p>

#### 3. Analyzing variance
##### We analyze the variance to derive optimal baselines to minimize the variance. We use g(τ) to denote grad-θ-log-π. In practice, we won't bother and we'll just use average reward.
<p align="center">tuitively
<img src="/images/77.png"><br/>
</p>

### (11) How the policy gradient is on-policy and what we can do about it?
#### On-policy means that every time you change your policy, you need to generate new samples in order to improve your policy further and you can't reuse old samples that came from other policies (even from older versions of your own policy). Now, every time you want to estimate your gradient, you need to calculate a sample-wise estimation of expectation under your current policy. If you apply that gradient and change your policy, then you cannot use those samples to get an unbiased estimation of this expectation because your samples now come from a different distribution. We just need to pay attention to the "τ~πθ(τ)" of expectation. The step1 of REINFORCE algorithm is necessary. If we skip step1, we're not getting the a real policy gradient and the policy will not improve.
<p align="center">tuitively
<img src="/images/78.png"><br/>
</p>

### (12) Off-policy learning & importance sampling
#### Since on-policy learning is so inefficient, we can actually turn policy gradients into a somewhat off-policy algorithm (textbook-off policy). We'll do that using importance sampling. Importance sampling is a simple and convenient technique that we can use to estimate expectations under one distribution when we only have acess to samples from a different distribution. 
<p align="center">
<img src="/images/79.png"><br/>
</p>

### (13) Deriving the policy gradient with importance sampling (no matter on-policy or off-policy)
#### Now we can actually use importance sampling to derive the policy gradient in an alternative way. This is an alternative way to derive policy gradient, starting from importance sampling and the sampling is always with respect to the old policy. When θ is not equal to θ', we can actully get off-policy policy gradient.
<p align="center">
<img src="/images/80.png"><br/>
</p>

### (14) The off-policy policy gradient
#### Note: when we take a very large number of numbers between 0 and 1 and multiply them together, we'll get really small total product. So we may get either gigantic or tiny weights in the equation, which may increase the variance of policy gradient. We then need to design baselines and so on which will be negated by the enormous variance that we'll get from those crazy importance weights. That's a big problem for importance sample policy gradient. We can do a few tricks like exploiting the causality - future actions don't affect current weight. By ignoring the part in the picture, we'll get different gradient but we can still improve policy. We are still in trouble with the small product.
<p align="center">
<img src="/images/81.png"><br/>
</p>

### (15) A first-order approximation for importance sampling (preview)
#### If θ and θ' are close to each other (KL divergence, πθ can be a recent old policy), we can ignore that part in the picture and the error that we get from deleting this term is small. We can implement πθ(a|s) but not πθ(s,a).
<p align="center">
<img src="/images/82.png"><br/>
</p>

### (16) Implementing policy gradient with automatic differentiation
#### A little bit of practical discussion. There's a very convenient way to implement policy gradients with standard automatic differentiation packages. We need to construct a computation graph using softwares like Tensorflow and Pytorch.
<p align="center">
<img src="/images/83.png"><br/>
</p>

<p align="center">
<img src="/images/84.png"><br/>
</p>

<p align="center">
<img src="/images/85.png"><br/>
</p>

### (15) Policy gradient in practice
<p align="center">
<img src="/images/86.png"><br/>
</p>

### (16) Review2
#### The importance weights are scaling in capital t. You can ignore parts of the weights which gives you an approximation (for now looks like a hack but it actually makes some mathematical sense). We don't naively just compute out all the grad-log-πθ, instead we ask the software to do it for us.
<p align="center">
<img src="/images/87.png"><br/>
</p>




