### CS285 Lecture6: Actor-Critic Algorithms
### (1) Today’s Lecture
#### 1. Improving the policy gradient with a critic - a better policy gradient usually means a lower variance policy gradient
#### 2. The policy evaluation problem
#### 3. Discount factors
#### 4. The actor-critic algorithm
#### 5. Goals:
##### • Understand how policy evaluation fits into policy gradients  
##### • Understand how actor-critic algorithms work
### (2) Recap: policy gradients
#### Most of what we'll discuss today is that we'll deal with better ways to estimate the "reward to go". So far in policy gradients, the "reward to go" was just estimated by using the actual rewards that you saw along that one trajectory that you executed. It is an estimate of the return that you'll get if you take this action a in state s and then follow the policy and that estimate. But in reinforcement learning, we're dealing with stochastic environments and also stochastic policies. So if you were toteleport yoursel back to the state take that same action and then run your policy agani, you'll probabily get a slightly different reward because even from the same state and from the same action, you might land in a different state at time step t+1. When estimaing Q hat, we really want the average of all those which were crudely approximating with a single sample.
<p align="center">
<img src="/images/88.png"><br/>
</p>

### (3) Improving the policy gradient
#### This will be a low variance gradient because if you're using a single sample to estimate the expectation, you'll compute your estimator multiple times and get slightly different rewards - the single sample will change. If you average together all possible trajectories, you'll get more consistant gradients. If we get a better policy gradient/a lower variance policy gradient, we can take bigger steps and learn faster. 
<p align="center">
<img src="/images/96.png"><br/>
</p>

### (4) What about the baseline?
#### 1. In practice, we always use a baseline version of policy gradient. We always use the average reward as the baseline b. But what are we averaging now? We can average all Q-values for different actions. 
<p align="center">
<img src="/images/99.png"><br/>
</p>

#### 2. But actually it turns out that it is perfectly allowed to actually use a state dependent baseline Q - V. There's a whole proof how the baseline is unbiased for the case the baseline depends on state s. Q - V quantifies how much better the action at is than the average action from the state because the value V is iterally defined as the average Q-value over all the actions distributed according to the policy. The value is positive for actions that are better than average and negative for actions for actions that are worse than average. We call the value "advantage A".
<p align="center">
<img src="/images/97.png"><br/>
</p>

<p align="center">
<img src="/images/100.png"><br/>
</p>

### (5) State & state-action value functions
#### Q-value function compute the total reward from taking at in state st. The previous version of Qhat - b is just a not very good estimate of advantage, a high variance single sample estimate of the advantage. All of today's lecture will be about how do we do a better job of estimating the quantities Q,V,A. 
<p align="center">
<img src="/images/98.png"><br/>
</p>

### (6) Value function fitting
#### 1. What do we fit and what do we fit it to? The first reward of the expected reward is just a scalar number. 
<p align="center">
<img src="/images/102.png"><br/>
</p>

<p align="center">
<img src="/images/101.png"><br/>
</p>

#### 2. Maybe it's OK if we're a little bit crude about how we evaluate the expectation. We can just use a single sample to estimate.
<p align="center">
<img src="/images/103.png"><br/>
</p>

<p align="center">
<img src="/images/104.png"><br/>
</p>

#### 3. Let's just fit Vπ(s)!
<p align="center">
<img src="/images/105.png"><br/>
</p>






