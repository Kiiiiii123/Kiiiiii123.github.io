### CS285 Lecture6: Actor-Critic Algorithms
### (1) Today’s Lecture
#### 1. Improving the policy gradient with a critic - a better policy gradient usually means a lower variance policy gradient
#### 2. The policy evaluation problem
#### 3. Discount factors
#### 4. The actor-critic algorithm
#### 5. Goals:
##### • Understand how policy evaluation fits into policy gradients  
##### • Understand how actor-critic algorithms work
### (2) Recap: policy gradients
#### Most of what we'll discuss today is that we'll deal with better ways to estimate the "reward to go". So far in policy gradients, the "reward to go" was just estimated by using the actual rewards that you saw along that one trajectory that you executed. It is an estimate of the return that you'll get if you take this action a in state s and then follow the policy and that estimate. But in reinforcement learning, we're dealing with stochastic environments and also stochastic policies. So if you were toteleport yoursel back to the state take that same action and then run your policy agani, you'll probabily get a slightly different reward because even from the same state and from the same action, you might land in a different state at time step t+1. When estimaing Q hat, we really want the average of all those which were crudely approximating with a single sample.
<p align="center">
<img src="/images/88.png"><br/>
</p>

### (3) Improving the policy gradient
#### This will be a low variance gradient because if you're using a single sample to estimate the expectation, you'll compute your estimator multiple times and get slightly different rewards - the single sample will change. If you average together all possible trajectories, you'll get more consistant gradients. If we get a better policy gradient/a lower variance policy gradient, we can take bigger steps and learn faster. 
<p align="center">
<img src="/images/96.png"><br/>
</p>

### (4) What about the baseline?
#### 1. In practice, we always use a baseline version of policy gradient. We always use the average reward as the baseline b. But what are we averaging now? We can average all Q-values for different actions. 
<p align="center">
<img src="/images/99.png"><br/>
</p>

#### 2. But actually it turns out that it is perfectly allowed to actually use a state dependent baseline Q - V. There's a whole proof how the baseline is unbiased for the case the baseline depends on state s. Q - V quantifies how much better the action at is than the average action from the state because the value V is iterally defined as the average Q-value over all the actions distributed according to the policy. The value is positive for actions that are better than average and negative for actions for actions that are worse than average. We call the value "advantage A".
<p align="center">
<img src="/images/97.png"><br/>
</p>

<p align="center">
<img src="/images/100.png"><br/>
</p>

### (5) State & state-action value functions
#### Q-value function compute the total reward from taking at in state st. The previous version of Qhat - b is just a not very good estimate of advantage, a high variance single sample estimate of the advantage. All of today's lecture will be about how do we do a better job of estimating the quantities Q,V,A. 
<p align="center">
<img src="/images/98.png"><br/>
</p>

### (6) Value function fitting - What do we fit?
#### 1. The first reward of the expected reward is just a scalar number. 
<p align="center">
<img src="/images/102.png"><br/>
</p>

<p align="center">
<img src="/images/101.png"><br/>
</p>

#### 2. If we have a good estimate of the value at time step t+1, maybe it's OK if we're a little bit crude about how we evaluate this expectation. We use a single sample estimate for just St+1, but still plug in the full value function at that time step. This is an approximately equal because we might have other St+1 if we try multiple times. If we don't want to fit a function approximator to something that takes in states and actions, it's actually almost enough to just fit the value if we're willing to pay a little bit of price for using that single sample estimate for st+1. The classic actor-critic algorithm will fit the value function Vπ but there're other actor-critic algorithms that do fit Qπ.
<p align="center">
<img src="/images/103.png"><br/>
</p>

<p align="center">
<img src="/images/104.png"><br/>
</p>

#### 3. Let's first fit Vπ(s) because taht way we have fewer inputs to deal with.
<p align="center">
<img src="/images/105.png"><br/>
</p>

### (7) Policy evaluation - what do we fit it to? 
#### If you have the value function you know how good your policy is because the reinforcement learning objective is just the value at the initial state. Or if you have multiple initial states then it's the expected value over the initial states. To perform policy evaluation we generate a bunch of samples and use those samples to test how good your policy is. This is called Monte Carlo policy evaluation.
<p align="center">
<img src="/images/107.png"><br/>
</p>

### (8) Monte Carlo evaluation with function approximation
#### We construct a training set and solve a supervied regression problem.
<p align="center">
<img src="/images/108.png"><br/>
</p>

### (9) Can we do better?
#### 1. Using a bootstrapped value estimate （自展估计）means using r + V
#### 2. Using a Monte Carlo estimate means doing the sum of rewards
<p align="center">
<img src="/images/109.png"><br/>
</p>

### (10) An actor-critic algorithm 
#### Put those things together to bulid a complete actor-critic algorithm. It's a type of policy gradient algorithm that uses value function approximation. The structure of the method looks like REINFORCE except we've added some additional steps. We can fit the neural network Vπ to single sample estimates. We can also fit it to bootstrap returns.
<p align="center">
<img src="/images/110.png"><br/>
</p>

<p align="center">
<img src="/images/111.png"><br/>
</p>

### (11) Aside: discount factors
#### The concept of discount factor deals with the problem of what happens when the episode length is goes to infinity but we can also use it with finite episodes. Rewards sooner is better than rewards later to our agent.
<p align="center">
<img src="/images/112.png"><br/>
</p>

### (11) Aside: discount factors for policy gradients
#### This is a lower variance estimate of option one. But we can apply causility and those distributed stuff to option two and we'll have different discounts for our grad-log-πs at different time steps. And what that means is that for later time steps the grad-log-π is just multiplied by smaller numbers, by rewards by heavily discounted.
<p align="center">
<img src="/images/113.png"><br/>
</p>

### (12) Which version is the right one?
#### Option two is not what we typically use. What's the mathematical interpretation of option one or which problem is it actually solving? It is not solving the MDP with the death state. It's doing something else. Option one is optimizing for a truly infinite horizon problem, one there's no probability of death. Doing with discount factor avoids having these infinite sums. Let's think about what happens as you increase gamma. As gamma goes from 0.99 to 0.9999, these sums become bigger. And as gamma approaches one, the sums approach infinite. But if the multiplier on the grad-log-π gets bigger, the variance also gets bigger. Gamma can also be seen as a way to limit variance. What we want is to maximize expected reward for infinite horizon problem without any discount but getting infinite returns means infinite variance and we cannot actually produce any reasonable estimate of our policy gradient. What we do is putting in a little hack that reduces our variance to a finite value. The lower gamma is, the lower our variance is. If gamma goes to zero, variance goes to zero because everything is zero. Gamma allows us to tradeoff between bias and variance.
<p align="center">
<img src="/images/114.png"><br/>
</p>

### (13) Actor-critic algorithms (with discount)
#### The online actor-critic algorithm can make updates every single time step without even generating full trajectories. But if we just implement this naively, it usually won't work. 
<p align="center">
<img src="/images/115.png"><br/>
</p>

### (14) Architecture design
#### The following are some implementing details for actor-critic algorithms. One choice we have to make is what kind of neural network architecture to choose. A very simple and straightforward choice is to have two totally separate neural nets. One takes in s and outputs estimate of the value, the second one takes in s and outputs the probabilities over the actions. This way does has some downsides, if the states are extremely high dimentional and extremely comlicated, we might want to share representations between our value function and policy. If we schoose to share net' weights, the trouble is that we have two very different gradients, one from the supervised regression and the other one from policy gradient, hitting the same shared layers. Figuring out how to balance those gradients so that one doesn't obliterate the other can be a little tricky. The variance of policy gradients tends to be much larger than the variance of supervised regression objective.
<p align="center">
<img src="/images/116.png"><br/>
</p>

### (15) Online actor-critic in practice
#### There's a problem with making online actor-critic works in practice - we need to take a gradient step with a mini batch size of one, which is a bad idea even in supervised learning. To solve this, we need to use parallel workers to get a batch size greater than one. We then get a synchronized parallel actor-critic algorithm where we have multiple workers essentially multiple simulators running in parallel collecting transitions (the action and the next state can be quite different but the underlying distributions are the same). Asynchronous parallel actor-critic algorithm is very simillar to asynchronous SGD, we would have a parameter server that keeps track of the parameters of our policy and value function. Once our workers take the step, they will perform this update and exchange parameters of the parameter server and so on. In practice, people usually use the asynchronous design because it tends to be faster and easier to implement and we don't deal with synchronization. The purpose of the parallism here is not to make it faster, it's actually to make it work at all because if we don't have parallel workers, we have to make these updates using a single sample batch (the variance is too high and we can't make any progress). This will make it go somewhat faster if our simulator is expensive because we're paralyzing simulators (not paralyzing learning). If we actually want it to go faster, we need to paralyze the learning and use multiple workers for gradient updates. 
<p align="center">
<img src="/images/117.png"><br/>
</p>

### (16) Critics as state-dependent baselines
#### A different way that we can use critics. Can we use a critic to get an estimator that is unbiased but still has lower variance? We could use our critic not directly to evaluate the advantage but just as a better baseline that is a replacement for b. 
<p align="center">
<img src="/images/118.png"><br/>
</p>

### (17) Control variates: action-dependent baselines
#### 1. Note: Subtracting value function is fine because value function depends on the state, subtracting Q-function is not correct but we can add an additional term to compensate for the error introduced by the fact that baseline depends on action and fix it up. The expected value of state-dependent baseline is zero and we don't need to compensate for it. The baseline here is called control variate.
#### 2. Why do we even bother with a lawless policy gradient stuff? We can choose an approximation of our estimated Q values that makes this expectation tractable to calculate. And there're a variety of things that can make it tractable to calculate. For example, if our Q-function is quadratic in the action and our policy is multivariate normal, the expected value of a quadratic function under normal distribution can be calculated analytically. The sample-wise estimation of Q hats cannot be calculated analytically, if we baseline with a quadratic Q function whose expected value can be analytically, we might get even lower variance. In general, with control variates, we choose some representation for our control variate (Q) here, whose expected value can be calculated more easily than using samples from real environment. There're a whole host of tricks to do this.
#### 3. The recommended paper as a starting point to learn about control variates in policy gradient methods. There're many different tricks that people have tried to plug in some kind of action-dependent term here, get the magnitude of this term to be really small, use this to take care of the bulk of the gradient calculation and then just keep this thing around to essentially make the whole thing unbiased. This is a very common area of research right now for improving the variance of policy gradients. 
<p align="center">
<img src="/images/119.png"><br/>
</p>

### (18) Eligibility traces & n-step returns
#### Why don't we interpolate the two advantage estimators and get a single estimator to control bias/variance tradeoff?  We can actually combine them. We can introduce some parameters that will control the bias/variance tradeoff more continuously than just a binary choice. As we go further into the future, our variance increases. Monte Carlo estimator is pretty useful for getting accurate values in the near term but completely useless for getting accurate values in the long term. The value-based estimators' bias might hurt us a lot for things that are very close by. But for things further in the future, their ability to integrate all of these different possible outcomes will result much better estimates. So we'd like to use the value function for further in the future and Monte Carlo estimates the sums of rewards for closer to our present time.
<p align="center">
<img src="/images/120.png"><br/>
</p>

### (19) Generalized advantage estimation
#### Do we have to just choose one n or could we do something softer? It's better if we can not commit to just a single value of n but actually  average together multiple n-step estimators. What we can do is essentially cut everywhere at once and then average them together. We use an expotional decay for the weights scheme. At every time step in the future, we get this linear interpolation: one minus lambda times the value for cutting, lambda times value for not cutting. We can actually show with a bit of algebra to make the formula much simpler.
<p align="center">
<img src="/images/121.png"><br/>
</p>

### (20) Review
<p align="center">
<img src="/images/122.png"><br/>
</p>







