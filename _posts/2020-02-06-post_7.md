### CS285 Lecture6: Actor-Critic Algorithms
### (1) Today’s Lecture
#### 1. Improving the policy gradient with a critic - a better policy gradient usually means a lower variance policy gradient
#### 2. The policy evaluation problem
#### 3. Discount factors
#### 4. The actor-critic algorithm
#### 5. Goals:
##### • Understand how policy evaluation fits into policy gradients  
##### • Understand how actor-critic algorithms work
### (2) Recap: policy gradients
#### Most of what we'll discuss today is that we'll deal with better ways to estimate the "reward to go", So far in policy gradients, the "reward to go" was just estimated by using the actual rewards that you saw along that one trajectory that you executed. But in reinforcement learning, we're dealing with stochastic environments and also stochastic policies. So if you were t teleport 
<p align="center">
<img src="/images/88.png"><br/>
</p>
