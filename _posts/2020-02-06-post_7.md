### CS285 Lecture6: Actor-Critic Algorithms
### (1) Today’s Lecture
#### 1. Improving the policy gradient with a critic - a better policy gradient usually means a lower variance policy gradient
#### 2. The policy evaluation problem
#### 3. Discount factors
#### 4. The actor-critic algorithm
#### 5. Goals:
##### • Understand how policy evaluation fits into policy gradients  
##### • Understand how actor-critic algorithms work
### (2) Recap: policy gradients
#### Most of what we'll discuss today is that we'll deal with better ways to estimate the "reward to go". So far in policy gradients, the "reward to go" was just estimated by using the actual rewards that you saw along that one trajectory that you executed. It is an estimate of the return that you'll get if you take this action a in state s and then follow the policy and that estimate. But in reinforcement learning, we're dealing with stochastic environments and also stochastic policies. So if you were toteleport yoursel back to the state take that same action and then run your policy agani, you'll probabily get a slightly different reward because even from the same state and from the same action, you might land in a different state at time step t+1. When estimaing Q hat, we really want the average of all those which were crudely approximating with a single sample.
<p align="center">
<img src="/images/88.png"><br/>
</p>

### (3) Improving the policy gradient
#### This will be a low variance gradient because if you're using a single sample to estimate the expectation, you'll compute your estimator multiple times and get slightly different rewards - the single sample will change. If you average together all possible trajectories, you'll get more consistant gradients. If we get a better policy gradient/a lower variance policy gradient, we can take bigger steps and learn faster. 
<p align="center">
<img src="/images/96.png"><br/>
</p>

### (4) What about the baseline?
#### In practice, we always use a baseline version of policy gradient. We always use the average reward as the baseline b. But what are we averaging now? We can average all Q-values for all states (the expected reward for all of different directions). But actually it turns out that it is perfectly allowed to actually use a state dependent baseline. There's whole proof how the baseline is unbiased for the case the baseline depends on state s.
<p align="center">
<img src="/images/97.png"><br/>
</p>

### (5) State & state-action value functions
<p align="center">
<img src="/images/98.png"><br/>
</p>





