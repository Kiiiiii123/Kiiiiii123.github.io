### CS285 Lecture6: Actor-Critic Algorithms
### (1) Today’s Lecture
#### 1. Improving the policy gradient with a critic - a better policy gradient usually means a lower variance policy gradient
#### 2. The policy evaluation problem
#### 3. Discount factors
#### 4. The actor-critic algorithm
#### 5. Goals:
##### • Understand how policy evaluation fits into policy gradients  
##### • Understand how actor-critic algorithms work
### (2) Recap: policy gradients
#### Most of what we'll discuss today is that we'll deal with better ways to estimate the "reward to go". So far in policy gradients, the "reward to go" was just estimated by using the actual rewards that you saw along that one trajectory that you executed. It is an estimate of the return that you'll get if you take this action a in state s and then follow the policy and that estimate. But in reinforcement learning, we're dealing with stochastic environments and also stochastic policies. So if you were toteleport yoursel back to the state take that same action and then run your policy agani, you'll probabily get a slightly different reward because even from the same state and from the same action, you might land in a different state at time step t+1. When estimaing Q hat, we really want the average of all those which were crudely approximating with a single sample.
<p align="center">
<img src="/images/88.png"><br/>
</p>

### (3) Improving the policy gradient
#### This will be a low variance gradient because if you're using a single sample to estimate the expectation, you'll compute your estimator multiple times and get slightly different rewards - the single sample will change. If you average together all possible trajectories, you'll get more consistant gradients. If we get a better policy gradient/a lower variance policy gradient, we can take bigger steps and learn faster. 
<p align="center">
<img src="/images/96.png"><br/>
</p>

### (4) What about the baseline?
#### 1. In practice, we always use a baseline version of policy gradient. We always use the average reward as the baseline b. But what are we averaging now? We can average all Q-values for different actions. 
<p align="center">
<img src="/images/99.png"><br/>
</p>

#### 2. But actually it turns out that it is perfectly allowed to actually use a state dependent baseline Q - V. There's a whole proof how the baseline is unbiased for the case the baseline depends on state s. Q - V quantifies how much better the action at is than the average action from the state because the value V is iterally defined as the average Q-value over all the actions distributed according to the policy. The value is positive for actions that are better than average and negative for actions for actions that are worse than average. We call the value "advantage A".
<p align="center">
<img src="/images/97.png"><br/>
</p>

<p align="center">
<img src="/images/100.png"><br/>
</p>

### (5) State & state-action value functions
#### Q-value function compute the total reward from taking at in state st. The previous version of Qhat - b is just a not very good estimate of advantage, a high variance single sample estimate of the advantage. All of today's lecture will be about how do we do a better job of estimating the quantities Q,V,A. 
<p align="center">
<img src="/images/98.png"><br/>
</p>

### (6) Value function fitting - What do we fit?
#### 1. The first reward of the expected reward is just a scalar number. 
<p align="center">
<img src="/images/102.png"><br/>
</p>

<p align="center">
<img src="/images/101.png"><br/>
</p>

#### 2. If we have a good estimate of the value at time step t+1, maybe it's OK if we're a little bit crude about how we evaluate this expectation. We use a single sample estimate for just St+1, but still plug in the full value function at that time step. This is an approximately equal because we might have other St+1 if we try multiple times. If we don't want to fit a function approximator to something that takes in states and actions, it's actually almost enough to just fit the value if we're willing to pay a little bit of price for using that single sample estimate for st+1. The classic actor-critic algorithm will fit the value function Vπ but there're other actor-critic algorithms that do fit Qπ.
<p align="center">
<img src="/images/103.png"><br/>
</p>

<p align="center">
<img src="/images/104.png"><br/>
</p>

#### 3. Let's first fit Vπ(s) because taht way we have fewer inputs to deal with.
<p align="center">
<img src="/images/105.png"><br/>
</p>

### (7) Policy evaluation - what do we fit it to? 
#### If you have the value function you know how good your policy is because the reinforcement learning objective is just the value at the initial state. Or if you have multiple initial states then it's the expected value over the initial states. To perform policy evaluation we generate a bunch of samples and use those samples to test how good your policy is. This is called Monte Carlo policy evaluation.
<p align="center">
<img src="/images/107.png"><br/>
</p>

### (8) Monte Carlo evaluation with function approximation
#### We construct a training set and solve a supervied regression problem.
<p align="center">
<img src="/images/108.png"><br/>
</p>

### (9) Can we do better?
#### 1. using a bootstrapped value estimate （自展估计）means using r + V
#### 2. using a Monte Carlo estimate means doing the sum of rewards
<p align="center">
<img src="/images/109.png"><br/>
</p>













