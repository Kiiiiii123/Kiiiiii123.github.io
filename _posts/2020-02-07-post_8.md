### CS285 Lecture7: Value Function Methods
### (1) Today’s Lecture
#### 1. What if we just use a critic, without an actor?
#### 2. Extracting a policy from a value function
#### 3. The Q-learning algorithm
#### 4. Extensions: continuous actions, improvements
#### 5. Goals:
##### • Understand how value functions give rise to policies
##### • Understand the Q-learning algorithm
##### • Understand practical considerations for Q-learning
### (2) Recap: actor-critic
<p align="center">
<img src="/images/123.png"><br/>
</p>

### (3) Can we omit policy gradient completely?
#### A lot of the trouble that we've been going through so far has been because the policy gradient has such high variance. All the stuff that we needed to do (fitting the critic, using n-step return estimators, all the business with baselines etc) is because the variance of policy gradient is high. We can just use the fitted value function in some way to get an implicit policy and avoid having to explicitly preform gradient steps on our policy parameters. If action a has a large advantage then we can just force our policy to take that action. The action the agent will take at state st and get with argmax advantage function will be at least as good as any action agent can get from sampling from our policy. We will not get worse from doing the argmax policy at all states. Estimating the advantage and using argmax helps us recover the policy. If we use argmax at all states at all times, we can only do better and cannot do worse. The new policy π' is better than policy π unless π is optimal. We then get a new way to design a reinforcement learning algorithm.
<p align="center">
<img src="/images/124.png"><br/>
</p>

### (4) Policy iteration
#### To get into the guts of how these kinds of methods will work, we need to first introduce the concept of policy iteration: step one is to figure out which actions are good and step two is to take the good ones. It's called policy iteration because we iterate between evaluating our policy and improving our policy by setting it to be the best one we can get from our current advantage. To evaluate advantage function, let's evaluate value function and there's a variety ways to do this.
<p align="center">
<img src="/images/125.png"><br/>
</p>

### (5) Dynamic programming
#### First, we should make some unreasonable assumptions to derive basic dynamic programming algorithms that do not work under kind of traditional RL settings but do work if we basically know everything about how the world works. We could imagine that we live in a small grid world kind of environment where the agent gets to walk around this grid. We can tabulate everything which means we can store full Vπ(s) in a table. We'll see later on how we can relax the assumptions.
<p align= "center">
<img src="/images/126.png"><br/>
</p>




