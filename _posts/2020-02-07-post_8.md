### CS285 Lecture7: Value Function Methods
### (1) Today’s Lecture
#### 1. What if we just use a critic, without an actor?
#### 2. Extracting a policy from a value function
#### 3. The Q-learning algorithm
#### 4. Extensions: continuous actions, improvements
#### 5. Goals:
##### • Understand how value functions give rise to policies
##### • Understand the Q-learning algorithm
##### • Understand practical considerations for Q-learning
### (2) Recap: actor-critic
<p align="center">
<img src="/images/123.png"><br/>
</p>

### (3) Can we omit policy gradient completely?
#### A lot of the trouble that we've been going through so far has been because the policy gradient has such high variance. All the stuff that we needed to do (fitting the critic, using n-step return estimators, all the business with baselines etc) is because the variance of policy gradient is high. We can just use the fitted value function in some way to get an implicit policy and avoid having to explicitly preform gradient steps on our policy parameters. If action a has a large advantage then we can just force our policy to take that action. The action the agent will take at state st and get with argmax advantage function will be at least as good as any action agent can get from sampling from our policy. We will not get worse from doing the argmax policy at all states. Estimating the advantage and using argmax helps us recover the policy. If we use argmax at all states at all times, we can only do better and cannot do worse. The new policy π' is better than policy π unless π is optimal. We then get a new way to design a reinforcement learning algorithm.
<p align="center">
<img src="/images/124.png"><br/>
</p>

### (4) Policy iteration
#### To get into the guts of how these kinds of methods will work, we need to first introduce the concept of policy iteration: step one is to figure out which actions are good and step two is to take the good ones. It's called policy iteration because we iterate between evaluating our policy and improving our policy by setting it to be the best one we can get from our current advantage. To evaluate advantage function, let's evaluate value function and there's a variety ways to do this.
<p align="center">
<img src="/images/125.png"><br/>
</p>

### (5) Evaluating Vπ(s) with dynamic programming
#### 1. First, we should make some unreasonable assumptions to derive basic dynamic programming algorithms that do not work under kind of traditional RL settings but do work if we basically know everything about how the world works. We could imagine that we live in a small grid world kind of environment where the agent gets to walk around this grid. We can tabulate everything which means we can store full Vπ(s) in a table. We'll see later on how we can relax the assumptions. Our previous policy in previous policy iteration has been updated with the argmax, so our current π is a deterministic policy and we can simplify the formula a little bit to get a simpler expression. The following discusses about a dynamic programming appraoch to get Vπ (assume that we know everything about the system). 
#### 2. A very interesting question: Do we lose something when we decide to give up on representing stochastic policies and start dealing only with deterministic policies? We can actually show that for any fully observed MDP, there exists a policy that is optimal and deterministic. This is not true for partially observed MDPs.
<p align= "center">
<img src="/images/126.png"><br/>
</p>

### (6) Policy iteration with dynamic programming
#### We can view the procedure here as a procedure that repeatedly imrpoves the policy, we can also view it as a procedure that repeatedly improves our value function. We are moving towards better and better policies, another way to look at is that we are moving towards better and better value functions. That means better value functions for better policies. In order to evaluate the value function, we need to know the value of the latest policy.
<p align= "center">
<img src="/images/127.png"><br/>
</p>

### (7) Value iteration - Even simpler dynamic programming
#### Maybe we can skip policy iteration and just improve the value function (value iteration). Policy π' comes from argmax of A and argmax A is actually equal to argmax of Q. They only differ by the term that we're subtracting and this term depends on the state. The value iteration will always converge to optimal if we have a tabular representation. We plug the piece of value iteration into our diagram.
<p align= "center">
<img src="/images/128.png"><br/>
</p>

### (8) Fitted value iteration
#### When we can't represent complicated function with a table (dimension of state is too high), we represent with a neural network. We are gonna have a neural net for V instead having a big table. We assume that we actually know the transition probabilities right now. In order to turn the fitted value iteration algorithm into an algorithm that we can actually use, we can't enumerate all the states (there's no need to know all the states and we can just sample) and need to get rid of the assumption that we know the transition probabilities.
<p align= "center">
<img src="/images/129.png"><br/>
</p>

### (9) What if we don’t know the transition dynamics?
#### 1. We need to try every possible action from all of our sampled states. We don't need to enumerate all states but we do need to enumerate all actions from every state we sampled. The trouble maker is that we need to know outcomes for different actions. We are going to do a little trick to get rid of the assumption.
#### 2. Let's get back to policy iteration and think how could we do policy iteration without knowing the transition dynamics? The trick is to change how step one works. We iterate directly on the Q-function and we'll store a number for every state and action. We can do policy iteration in terms of Q-function tables instead of value function tables. The Q-function can be fitted using samples.
<p align= "center">
<img src="/images/130.png"><br/>
</p>

### (10) Can we do the "max" trick again? 
#### 1. Before, what we did is we took our policy iteration with value function, skipped explictly representing the policy and sticked in a max over a into our value updates. This gave us an algorithm that didn't require explictly keeping track of the policy. Here we need a "max" to try all different actions.
#### 2. Let's go to our Q-function version of policy iteration. There' a hiding "max" - the V is still the max over actions of Q-value. We are going to approximate the expected value of V(s') bu taking a sample of next state. This "max" doesn't require us to try every possible action through the dynamics, it just requires us to plug every possible action into Q-function estimator. Now what we need to learn is Q in place of V and recover V from Q by maximizing over all the actions which iterally amounts just plugging different actions into our function approximator.
#### 3. An interesting question: how do we deal with a setting where the actions are continuous? Learning a Q-function of continuous actions is actually not a big deal, the big deal is doing the maximization corresponding to the optimization problem which can be solved in a variety ways. 
#### 4. This method even works for off-policy samples. There's only one network and we don't need a separate actor or critic network. The downside of this method is that once we start using neural nets in place of tables, unfortunately this method doesn't converge. We'll find out how to use it properly but we should first find out why it doesn't converge.
<p align= "center">
<img src="/images/131.png"><br/>
</p>

### (11) Fitted Q-iteration
#### Summarize the complete algorithm and just put those pieces together.
<p align= "center">
<img src="/images/132.png"><br/>
</p>

### (12) Review1
<p align= "center">
<img src="/images/133.png"><br/>
</p>

### (13) Why is this algorithm off-policy?
#### An interesting question: how do we decide when to exit the loop between step two and step three and why should we jump back out to step one? The reason to jump back to step one is that our initial data needs to be collected using some initial policy (like random policy) and if our initial policy is very bad, we might never see some of the interesting states in our system. 
<p align= "center">
<img src="/images/134.png"><br/>
</p>

### (14) What is fitted Q-iteration optimizing?
#### Policy gradient is optimizing the expected return and making expected return bigger. Actor-critic is another way to approximate the policy gradient and optimize the ecpected return. For fitted Q-iteration optimization (tabular representation), we are minimizing bellman error which is basically the difference between target values and current values, we get the optimal Q-function and policy when the bellman error is zero. Most guarantees are lost when we leave the tabular case (e.g., when we use neural network function approximation).
<p align= "center">
<img src="/images/135.png"><br/>
</p>

### (15) Online Q-learning algorithms
#### We can also derive an on-line version of Q-learning algorithm. Notice the loop arrow. In step one we only collect one transition. Step three is to take one gradient step on the difference between current Q-value and the target value just computed. We take one gradient step every single transition and don't store any transitions. It's a special case of the full fitted Q-iteration algorithm. The batch size is one and it's not a good idea, it'll make the training progress not stable.
<p align= "center">
<img src="/images/136.png"><br/>
</p>

### (16) Exploration with Q-learning
#### Which policy should we use for step one? We want to select the policy for step one that is usually a bit more random than just the deterministic argmax policy. We have a few choices. One is the epsilon-greedy policy, the other is the Boltzmann exploration. We'll discuss exploration in more detail in a later lecture. For now, epsilon-greedy policy is a simple and straightforward choice.
<p align= "center">
<img src="/images/137.png"><br/>
</p>

### (17) Review2
<p align= "center">
<img src="/images/138.png"><br/>
</p>

### (18) Value function learning theory
#### 1. Let's talk about the convergence properties of value iteration. If it does converge, it converges to what? 
#### 2. We first simplify our notation a little bit by defining the bellman backup operator B. The operator B operates on a value function (it's a table right now and we can see it as a vector). We represent value functions as vectors and represent rewards for particular actions as vectors. Then we can see an expression in terms of linear algebra. We evaluate this for every possible action, get a bunch of vectors for every action and then we take an element-wise max. One of the things that we can show is that the optimal value function V* is a fixed point of B, it means that when we put the operator to the vector, we get back the same vector. V* is also the value function for the optimal policy. We know that this operator has a fixed point which is the optimal value function, now the real question is does it converge that fixed point? The fixed point always exists, it's always unique and corresponds to the optimal policy. We haven't proven that.
<p align= "center">
<img src="/images/139.png"><br/>
</p>

#### 3. Will we reach the fied point? We cna prove that value iteration reaches V* because is a contraction, it means that for any V and V bar (two vectors), the difference between the vectors we get by applying B to both of them is less than or equal to gamma times the difference between V and V bar. That is after applying B to V and V bar, they get closer to each other, hence contraction. Something important to note here is that the difference here is quantified in terms of the L-infinity norm. If the factor gamma is one, there's no contraction any more, but for any value less than one, the vectors get closer together. We can actually immediately recover a proof of convergence for value iteration. We simply use the fact that V* is a fixed point of B. We choose V bar to be the V*, then BV* is equal to V*, and we substitute in V* for V bar in this euqation. From the definition of contraction nad the knowledge that V* is a fixed point of B, we recover the equation at the bottom. What that implies is that if we apply the operator B to any vector V, it will become closer to V* in terms of the L-infinity norm. From this we can actually get a proof of convergence.
<p align= "center">
<img src="/images/140.png"><br/>
</p>

### (19) Non-tabular value function learning

### (20) What about fitted Q-iteration?

### (21) But… it’s just regression!
### (22) A sad corollary
### (23) Review3




