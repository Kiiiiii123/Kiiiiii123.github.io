### CS285 Lecture7: Value Function Methods
### (1) Today’s Lecture
#### 1. What if we just use a critic, without an actor?
#### 2. Extracting a policy from a value function
#### 3. The Q-learning algorithm
#### 4. Extensions: continuous actions, improvements
#### 5. Goals:
##### • Understand how value functions give rise to policies
##### • Understand the Q-learning algorithm
##### • Understand practical considerations for Q-learning
### (2) Recap: actor-critic
<p align="center">
<img src="/images/123.png"><br/>
</p>

### (3) Can we omit policy gradient completely?
#### A lot of the trouble that we've been going through so far has been because the policy gradient has such high variance. All the stuff that we needed to do (fitting the critic, using n-step return estimators, all the business with baselines etc) is because the variance of policy gradient is high. We can just use the fitted value function in some way to get an implicit policy and avoid having to explicitly preform gradient steps on our policy parameters. If action a has a large advantage then we can just force our policy to take that action. The action the agent will take at state st and get with argmax advantage function will be at least as good as any action agent can get from sampling from our policy. We will not get worse from doing the argmax policy at all states. Estimating the advantage and using argmax helps us recover the policy. If we use argmax at all states at all times, we can only do better and cannot do worse. The new policy π' is better than policy π unless π is optimal. We then get a new way to design a reinforcement learning algorithm.
<p align="center">
<img src="/images/124.png"><br/>
</p>

### (4) Policy iteration
#### To get into the guts of how these kinds of methods will work, we need to first introduce the concept of policy iteration: step one is to figure out which actions are good and step two is to take the good ones. It's called policy iteration because we iterate between evaluating our policy and improving our policy by setting it to be the best one we can get from our current advantage. To evaluate advantage function, let's evaluate value function and there's a variety ways to do this.
<p align="center">
<img src="/images/125.png"><br/>
</p>

### (5) Evaluating Vπ(s) with dynamic programming
#### 1. First, we should make some unreasonable assumptions to derive basic dynamic programming algorithms that do not work under kind of traditional RL settings but do work if we basically know everything about how the world works. We could imagine that we live in a small grid world kind of environment where the agent gets to walk around this grid. We can tabulate everything which means we can store full Vπ(s) in a table. We'll see later on how we can relax the assumptions. Our previous policy in previous policy iteration has been updated with the argmax, so our current π is a deterministic policy and we can simplify the formula a little bit to get a simpler expression. The following discusses about a dynamic programming appraoch to get Vπ (assume that we know everything about the system). 
#### 2. A very interesting question: Do we lose something when we decide to give up on representing stochastic policies and start dealing only with deterministic policies? We can actually show that for any fully observed MDP, there exists a policy that is optimal and deterministic. This is not true for partially observed MDPs,
<p align= "center">
<img src="/images/126.png"><br/>
</p>

### (6) Policy iteration with dynamic programming
#### We can view the procedure here as a procedure that repeatedly imrpoves the policy, we can also view it as a procedure that repeatedly improves our value function. 
<p align= "center">
<img src="/images/127.png"><br/>
</p>

### (7) Value iteration - Even simpler dynamic programming
#### Maybe we can skip policy iteration and just improve the value function (value iteration). Policy π' comes from argmax of A and argmax A is actually equal to argmax of Q. They only differ by the term that we're subtracting and this term depends on the state. The value iteration will always converge to optimal if we have a tabular representation.
<p align= "center">
<img src="/images/128.png"><br/>
</p>

### (8) Fitted value iteration
#### When we can't represent complicated function with a table (dimension of state is too high), we represent with a neural network. We are gonna have a neural net for V instead having a big table. In order to turn the fitted value iteration algorithm into an algorithm that we can actually use, we can't enumerate all the states (there's no need to know all the states and we can just sample) and need to get rid of the assumption taht we know the transition probabilities.
<p align= "center">
<img src="/images/129.png"><br/>
</p>

### (9) What if we don’t know the transition dynamics?
#### We need to try every possible action from all of our sampled states. We don't need to enumerate all states but we do need to enumerate all actions from every state we sampled.
<p align= "center">
<img src="/images/130.png"><br/>
</p>









