### Cross entropy
### (1) The amount of information
#### The amount of information in a message has a lot to do with its uncertainty. A sentence that requires a lot of external information to be certain is said to be informative. For example, if you hear "it's snowing in xishuangbanna, yunnan", you need to check the weather forecast, ask local people and so on (because it never snows in xishuangbanna, yunnan). On the contrary, if you are told that "people eat three meals a day", the information of the message is very small, because the certainty of the message is very high.

#### Then we can define the information amount of event x0 as follows (where p(x0) represents the probability of event x0):
<p align="center">
<img src="/images/90.png"><br/>
</p>

<p align="center">
<img src="/images/89.jpg"><br/>
The probability is always between 0 and 1, -log(x) as shown above
</p>

### (2) Entropy
#### The amount of information is for a single event, but the reality is that a single event can happen in a number of ways. For example, a roll of dice can happen in six ways.Entropy is a measure of the uncertainty of a random variable, the amount of information expected from all possible events.The formula is as follows:
<p align="center">
<img src="/images/91.png"><br/>
n represents the total number of situations in which an event is likely to occur
</p>

#### 其中一种比较特殊的情况就是掷硬币，只有正、反两种情况，该种情况（二项分布或者0-1分布）熵的计算可以简化如下：
<p align="center">
<img src="/images/92.png"><br/>
p(x)代表掷正面的概率，1-p(x)则表示掷反面的概率（反之亦然）
</p>

### (3) 相对熵
#### 相对熵又称KL散度，用于衡量对于同一个随机变量x的两个分布p(x)和q(x)之间的差异。在机器学习中，p(x)常用于描述样本的真实分布，例如[1,0,0,0]表示样本属于第一类，而q(x)则常常用于表示预测的分布，例如[0.7,0.1,0.1,0.1]。显然使用q(x)来描述样本不如p(x)准确，q(x)需要不断地学习来拟合准确的分布p(x)。

#### KL散度的公式如下，KL散度的值越小表示两个分布越接近：
<p align="center">
<img src="/images/93.png"><br/>
n表示事件可能发生的情况总数
</p>

### (4) 交叉熵
#### 我们将KL散度的公式进行变形，得到：
<p align="center">
<img src="/images/94.png"><br/>
</p>

#### 前半部分就是p(x)的熵，后半部分就是我们的交叉熵：
<p align="center">
<img src="/images/95.png"><br/>
</p>

#### 机器学习中，我们常常使用KL散度来评估predict和label之间的差别，但是由于KL散度的前半部分是一个常量，所以我们常常将后半部分的交叉熵作为损失函数，其实二者是一样的。

