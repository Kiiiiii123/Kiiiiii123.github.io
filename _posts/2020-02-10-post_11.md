### CS285 Lecture 8: Deep RL with Q-functions

### (1) Today’s Lecture

#### 1. How we can make Q-learning work with deep networks

#### 2. A generalized view of Q-learning algorithms

#### 3. Tricks for improving Q-learning in practice

#### 4. Continuous Q-learning methods

#### 5. Goals:

- ##### Understand how to implement Q-learning so that it can be used with complex function approximators

- ##### Understand how to extend Q-learning to continuous actions

### (2) Recap: Q-learning

<p align="center">
<img src="/images/147.png"><br/>
</p>

### (3) What's wrong with online Q-learning?

#### We can't prove that it converges. Like online version of actor-critic algorithm, we need to use mini batches of size greater than one for Q-learning. Step three isn't just gradient descent because there's a moving target y which depends on Φ. Otherwise, our samples are highly correlated (temporally sequential samples are not iid) because the way we collected them is by executing some policy in an environment. 

<p align="center">
<img src="/images/148.png"><br/>
</p>

### (4) Correlated samples in online Q-learning

#### Let's work on those correlated samples first and put the other problem (the target value is always changing) aside. With the correlated samples, instead of learning something, we're just repeatedly overfitting to our local neighborhood and not actually getting anywhere. We could solve this problem in a few ways. The first solution is implementing a parallel version of Q-learning, where we have multiple workers and do updates like this. We're still correlated in time but at least now we have batches of size more than one. That's the same as for actor-critic. It still doesn't fully resolve the correlation problem. While for actor-critic we didn't have any other choice because actor-critic had to be on-policy, with Q-learning we actually have a better option. We could do the parallel thing but there's a much simpler and much better solution to this issue (not available to us for on-policy actor-critic).   

<p align="center">
<img src="/images/149.png"><br/>
</p>

### (5) Another solution: replay buffers

#### Instead of literally interacting with the environment and collecting a batch of data using epsilon-greedy version of our latest policy, just load a batch of data from some past policy that we had. We can reload multiple times and reuse it. We need to periodically feed the replay buffer.

<p align="center">
<img src="/images/150.png"><br/>
</p>

<p align="center">
<img src="/images/151.png"><br/>
</p>

### (6) Full Q-learning with replay buffer - putting it together

<p align="center">
<img src="/images/152.png"><br/>
</p>

### (7) What's wrong?

#### The moving target is still a problem. 

<p align="center">
<img src="/images/153.png"><br/>
</p>

### (8) Q-learning and Regression

#### How Q-learning actually relates to regression? The full Q-learning with replay buffer looks like taking the gradient of a regression loss but that's not true. In step three of the fitted Q-iteration algorithm, it's actually written out as a regression problem. We'll find the Φ that minimizes the regression loss. So there's a well-defined supervised learning procedure  which will converge to a good policy. In step three of more online methods, we just do one gradient step of that. We can maybe import a lesson into the more online algorithm up top to get it to be more stable and suffer less from the smoothing target problem.  

<p align="center">
<img src="/images/154.png"><br/>
</p>

### (8) Q-learning with target networks

#### The difference now is that when we compute our target value we don't use QΦ, we use QΦ' instead. Φ' is some other parameter vector. Changing Φ doesn't change target values. We can just use the Q-function we had 10000 steps ago. We do that much less often so that most of the time in inner loop is trying to hit a moving target. 

<p align="center">
<img src="/images/155.png"><br/>
</p>

### (9) "Classic" deep Q-learning algorithm (DQN)

#### This method uses both replay buffer and target network.  How do we initialize the target network? In practice, initializing the parameters randomly is reasonable.

<p align="center">
<img src="/images/156.png"><br/>
</p>

### (10) Alternative target network

#### An alternative is that we can keep lag the same always. The alternative is very similar to something called Polyak averaging. Instead of actually updating Φ’ to be equal to Φ every 10000 steps, we can update it every single step but have a damping term on it. 

<p align="center">
<img src="/images/157.png"><br/>
</p>

### (11) Fitted Q-iteration and Q-learning

#### We have the target network updates in an inner/outer loop, the data being collected less/more often and the target value being updated more/less often. 

<p align="center">
<img src="/images/158.png"><br/>
</p>

### (12) A more general view

####  The more general view is that we have process one to collect data for dataset of transitions. Our buffer will fill up, so we need some way to evict old data, a very common choice is just to use a FIFO queue. We have another process two that updates the target parameters. This is a much slower process that will grab our current parameters and just copy them into the target parameters (or Polyak averaging).  Process three is the training process. It fetches data from replay buffer, fetches target parameters and use them to update current parameters. This actually updates our Q-function. These three processes can be treated as just three concurrent things that we can speed up or slow down however we want to obtain different versions of this algorithm. We can also implement this algorithm as a completely parallel system.

<p align="center">
<img src="/images/159.png"><br/>
</p>

<p align="center">
<img src="/images/160.png"><br/>
</p>

### (13) Are the Q-values accurate?

#### Now let's study some questions about deep Q-learning and learn about some tricks that we can use to make it work even better. The first question is: are the Q-values accurate? Q-value is a numerical prediction about our future reward. Do these Q-learning algorithms learn accurate predictions? 

<p align="center">
<img src="/images/161.png"><br/>
</p>

#### Are they actually numerically accurate? The following picture shows that on all four of these games the anticipated reward is actually much larger numerically than the actual reward obtained by the agent. This doesn't contradict what we saw in the last picture. Numerically the predicted reward seems to be much bigger than real reward. The predicted rewards still correlate with the right outcome. Worse actions have lower numbers and better actions have bigger numbers. Why does Q-function expect more rewards than the agent is actually going to get (or why overestimation)?  

<p align="center">
<img src="/images/162.png"><br/>
</p>

### (14) Overestimation in Q-learning

####  The "max" in target value calculation is the problem.

<p align="center">
<img src="/images/163.png"><br/>
</p>

### (15) Double Q-learning

#### We want to fix this problem which due to the fact that the same noise affects the argmax and the value. We can't have a perfect noiseless Q-function because errors always exists. The idea is that we don't use the same network to choose the action as the one that we actually use to calculate the value. What if we have one network that we use for the argmax and a different network that we use to evaluate the actual value for that argmax?  That's Double Q-learning. 

<p align="center">
<img src="/images/164.png"><br/>
</p>

### (16) Double Q-learning in practice

#### Where to get two Q-functions? We can just use the current and target networks.

<p align="center">
<img src="/images/165.png"><br/>
</p>

### (17) Multi-step returns

#### Let's bring up our Q-learning target calculation again. If our Q-function estimation is pretty good , most of our learning comes from the second term because it's the bigger term.  But if the Q-function is terrible or random (initial state), most of   the reward signal comes from the first term. The first term only gives us the reward for one time step but we'd like to learn how to behave intelligently for many time steps. So early on in training when our Q-function is bad, we're only learning about the number of first term, the reward for only one time step. We're not learning very much. We have to wait for a little while until the second term gets close enough to the true Q-values. Can we construct multi-step targets, like in actor-critic? Yes. But is it on-policy now? Our rewards in the equation came from some trajectory. If N is very large, the value of the first term is not the value of our current policy, that's the value of whatever policy collect the data. The sum of the first term estimates the return for a particular policy, so now we are not really off-policy any more.  

<p align="center">
<img src="/images/166.png"><br/>
</p>

### (18) Q-learning with N-step returns

#### Now we have a less biased value when Q-values are inaccurate, typically faster learning especially early on, but Q-learning is only actually correct when we have on-policy data. To really make this correct, we need transitions that actually come from our latest policy.

<p align="center">
<img src="/images/167.png"><br/>
</p>

### (19) Q-learning with continuous actions - option one

#### The problem with continuous actions is this "max" because we have infinite actions. It's not just the "max" in policy, the "max" in target value is arguably more problematic because inner loop of training. We have a number of different choices if we want to use Q-learning in continuous action spaces. The first option is the optimization option. It basically says let's use some optimization procedure to calculate this "max". We could do like gradient-based optimization (like literally run gradient descent on a) but this isn't a good idea because we have to do it in the inner loop of training to calculate our target value, which tends to be pretty slow. There are a few things we can exploit to make this optimization a little faster. 

<p align="center">
<img src="/images/168.png"><br/>
</p>

### (20) Q-learning with stochastic optimization

#### Parameter space for deep neural nets are huge, but action space is usually not that huge. This allows us to use fairly simple stochastic optimization method, which is to just approximate the "max" by a "max" of a discrete set of actions. We just pick some actions at random, evaluate Q-value of all of them and just pick the best one. It's not a good optimization algorithm but it's really fast and really easy to implement. If our problem is overestimation anyway, we don't need to optimize so hard, maybe just a little bit of optimization will do. But it's very easy to improve on this method building on the same building blocks. We can try some more accurate stochastic optimization solutions of course.  These methods work well for up to about 40 dimensions.   

<p align="center">
<img src="/images/169.png"><br/>
</p>

### (21) Easily maximizable Q-functions - option two

#### Option two is to use function class that is easy to optimize. Besides neural nets, there are other function approximators we can choose for which is possible to analytically calculate the "max". One such function approximator is quadratic function. If we have a quadratic function, figuring out the optimum is easy by solving a linear system. We can represent our Q-function as basically quadratic form where the matrix and vector are produced by a neural network. The neural network outputs a vector μ, a matrix P and a scalar V. This particular thing is called NAF, whose argmax is just μ corresponding to the Q-value V. We don't have to implement any fancy optimization or make any changes to original algorithm. It's just as efficient as Q-learning, but it has a pretty substantial downside which is that our Q-function now has to be quadratic in the action and we'll lose some representational power. If our actual real Q-function is highly non-quadratic in the action, then this thing might not do very well, but if we have a system has relatively simple actions but complex states, this can be a very convenient way to do Q-learning with continuous actions.     

<p align="center">
<img src="/images/170.png"><br/>
</p>

### (22) Q-learning with continuous actions - option three

#### Option three is to learn am approximate maximizer. We're going to learn another network that performs the optimization work. The particular algorithm  is called DDPG. This paper presents the method as a deterministic actor-critic algorithm, but we can also interpret is as a kind of approximate Q-learning.   

<p align="center">
<img src="/images/171.png"><br/>
</p>

#### Pseudocode for procedure of DDPG is as the following.

<p align="center">
<img src="/images/172.png"><br/>
</p>

### (23) Simple practical tips for Q-learning

<p align="center">
<img src="/images/173.png"><br/>
</p>

### (24) Advanced tips for Q-learning

<p align="center">
<img src="/images/174.png"><br/>
</p>

### (25) Review

<p align="center">
<img src="/images/175.png"><br/>
</p>



