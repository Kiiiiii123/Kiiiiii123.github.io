### CS285 Lecture9: Advanced Policy Gradients

### (1) Today’s Lecture

#### 1. Why does policy gradient work?

#### 2. Policy gradient is a type of policy iteration

#### 3. Policy gradient as a constrained optimization

#### 4. From constrained optimization to natural gradient

#### 5. Natural gradients and trust regions

#### 6. Goals:

- ##### Understand the policy iteration view of policy gradient

- ##### Understand how to analyze policy gradient improvement

- ##### Understand what natural gradient does and how to use it 

### (2) Recap: policy gradients

#### The REINFORCE algorithm is the most basic policy gradient algorithm. 

<p align="center">
<img src="/images/216.png"><br/>
</p>

### (3) Why does policy gradient work?

#### 1. We can replace Q-value with advantage. We can think the algorithm at a very high level. Step one estimate A hat for our current policy, step two somehow use A hat to get a better policy π‘ and then repeat the process.

#### 2. It looks familiar. From the training procedure of the policy gradient algorithm, we can see the basic recipe behind a policy iteration method. In the general scheme, step one evaluates how good our policy is and step two uses the estimate of how good the policy is to make it better. 

#### 3. In the policy iteration algorithm, we first evaluate the advantages of our policy and then use them to get a better policy. Take the Q-learning method as an example: we can get a better policy simply by taking the "argmax". So the policy gradient and policy iteration are quite similar and we can actually frame the policy gradient as a type of policy iteration. 

#### 4. How is policy gradient different from policy iteration? A gradient doesn't maximize something by itself. It may take many steps of gradient descent to get something optimal. 

<p align="center">
<img src="/images/219.png"><br/>
</p>

### (4) Policy gradient as policy iteration

#### 1. Here we'll derive how policy gradient is in fact doing policy iteration. We start by analyzing the policy improvement objective. The objective of reinforcement learning is the expected total reward of a policy - J(θ). J(θ') - J(θ) is the improvement of objective, which is the difference between the expected reward for a new parameter vector θ’ and our old parameter vector θ.

####  2. The starting point for the equivalence between policy gradient and policy iteration is that policy improvement is equal to the expected value of the advantage of our policy πθ under the trajectory distribution of our new policy πθ' (claim).  This is interesting to us because it's what we would expect to see in a policy iteration algorithm. In a policy iteration algorithm, we'd expect to see the following structure: estimate the advantage of our current policy and then maximize that advantage with respect to the new policy. More specifically, the structure we would expect in a policy iteration algorithm is step one: figure out what Aπθ is, step two: maximize this quantity basically get the new policy to take the actions for which Aπθ is big. All policy iteration methods do some variant of this. If we can show the J(θ') - J(θ) is equal to the policy improvement objective (advantage) in policy iteration, then we are on a way to show an equivalence between policy gradient and policy iteration.

#### 3. The proof is as follows. Notes: An expectation with respect to s0 can be equivalently written as expectation with respect to any trajectory distribution because any trajectory distribution for any policy has P(s0) as its initial state marginal. 

<p align="center">
<img src="/images/220.png"><br/>
</p>

#### 4. How to actually optimize this since we still have the expectation under πθ’? That's not good because it's very difficult to calculate a gradient without generating  additional samples from πθ’. We can write it out in the other form and apply the importance sampling trick from before. However we still have the  θ’ in the outer expectation, which is preventing us from using samples from πθ to optimize the objective. We have not done yet! 

<p align="center">
<img src="/images/221.png"><br/>
</p>

### (5) Can we just ignore the distribution mismatch?

#### 1. We haven't made any approximation so far. If we could just write it like this with a Pθ  there, θ' would only be in one spot where it's easy to manage. 

#### 2. Why do we want this to be true? If this was true, the difference between J(θ') and J(θ) is approximately equal to A bar(θ'), whose gradient we can calculate with respect to θ', without generating any samples. We can figure out the θ' that maximizes the A bar. If only that right-hand side was approximately equal to the left-hand side, we could implement the improvement step in policy iteration. 

#### 3. We really want this to be true, but is it true and when is it true? Our claim is almost seems obvious but if we want to derive convergent well-defined algorithms, we should prove that this is true rather than merely asserting it. 

<p align="center">
<img src="/images/222.png"><br/>
</p>

### (6) Bounding the distribution change

#### 1. To prove the claim, we'll start with a simple case and then go to a more advanced case. 

#### 2. In the simple case, we assume that πθ is a deterministic policy to make the derivation much easier. We define the notion of "close", which we have see in the imitation learning. Back from our imitation learning lecture, we had a relationship as follows. The first term in the right-hand side comes from the fact that with some probability our new policy πθ’ might do exactly the same thing as πθ. The probability is one minus epsilon every time step. Then we can bound the total variation divergence between Pθ’ and Pθ by two times the coefficient. With the useful identity, we can also bound it by two times epsilon times t. This is not a good bound because it gets bigger and bigger as epsilon gets bigger, but it's still a bound and for a small enough epsilon the state distributions will be close enough together.  

<p align="center">
<img src="/images/223.png"><br/>
</p>

#### 3. We'd like to derive the general case where πθ is an arbitrary distribution. We need to define a new notion of "close" in this case. πθ’ is close to πθ if the sum of the absolute values of differences between their probabilities is bounded by epsilon. We still need a useful lemma. Then we can directly reuse everything that we had on the previous slide. 

<p align="center">
<img src="/images/224.png"><br/>
</p>

#### 4. For any function f(st), we can bound its expectation under Pθ' by its expectation under pθ minus 2εt times the maximum value that function takes on. It's useful to us because if ε is very small, the second term might be negligible. If we optimize the expectation under Pθ and improve the bottom objective by more than the constant, we'll make our policy better.

<p align="center">
<img src="/images/225.png"><br/>
</p>

### (7) What are we at so far?

#### If we can maximize the following with respect to θ‘, we'll get a better policy, but only πθ' doesn't get too far away from πθ. We quantified too far away in terms of total variation divergence, then we can try to solve a constrained optimization problem, maximize that objective with respect to θ' subject to a constraint that the total variation divergence between πθ’ and πθ is less than or equal to ε. If we choose ε to be small enough, then we can get an improvement.

<p align="center">
<img src="/images/226.png"><br/>
</p>

### (8) A more convenient bound

#### Let's first come up with a slightly more convenient bound. Total variation divergence is very convenient for theoretical analysis but it's very inconvenient if we want to implement the algorithm, especially when we have continuous actions because calculating toleration divergence between two distributions is quite a pain. We'd like to use a more convenient notion of divergence. The total variation divergence is bounded by the square root of 1/2 times the KL divergence. So the KL divergence therefore bound the state marginal difference.  KL divergence is defined as the expectation under the first distribution p1 of the log of the ratio between the two distributions. We will prefer to use these KL divergences because many commonly used parameterizations for distributions are easy to calculate KL divergence. 

<p align="center">
<img src="/images/227.png"><br/>
</p>

### (9) How do we optimize the objective?

#### Now we have the same objective but slightly different constraint which is that the KL divergence between the policies is bounded by epsilon. We should start to think about how to optimize it now.  

<p align="center">
<img src="/images/228.png"><br/>
</p>

### (10) How do we enforce the constraint?

#### We can write out the lagrangian of our constraint optimization problem. If we pick the right λ, then optimizing lagrangian will give us the right answer. We can consider the following practical algorithm (an instance of dual gradient descent) and implement it. The intuition behind this is if our constraint is violated too much, meaning the KL divergence is larger than ε, we increase the λ. 

<p align="center">
<img src="/images/229.png"><br/>
</p>

### (11) How (else) do we optimize the objective?

#### 1. One of the ways that we can approximately optimize some objective is we can take a Taylor expansion of that objective and optimize that instead. If we have some complicated function like the following curve, and we know that we only care about optimizing that function within some region, if the region is small enough, then inside the region the Taylor expansion of our function will probably close enough to the original function, then we can optimize the Taylor expansion instead. Let's just do the simplest first-order Taylor expansion, our objective becomes the gradient of A bar transpose times θ' - θ subject to the constraint that the divergence is less than or equal to ε. The constraint also makes sure that we're in a small enough region and the Taylor expansion is reasonable. Our objective now has become linear (much simpler), but we have to calculate that gradient.  

<p align="center">
<img src="/images/230.png"><br/>
</p>

#### 2. We now have a linear objective and a KL divergence constraint. We are getting to something that looks much more like a policy gradient (at least the objective has a policy gradient). We find that calculating the gradient of A bar at θ is much simpler than at θ' because the importance weights vanish. We get exactly the normal policy gradient now! 

<p align="center">
<img src="/images/231.png"><br/>
</p>

#### 3. Can we just use the gradient then? We want to just do the gradient ascent. We claim that the gradient ascent maximizes the linearized objective subject to the constraint that the L2 distance between θ and θ' is bounded by some number. We can view this as a constraint optimization problem in parameter space. 

<p align="center">
<img src="/images/232.png"><br/>
</p>

#### 4. The above constraint is not good because we want to constrain the KL divergence between the policies not on the Euclidean distance between parameter vectors. Different parameters will change the distribution by different amounts. Some parameters change probabilities. Now we'll do the Taylor expansion on the constraint. If the KL divergence is small, we can approximate it with a Taylor expansion. We're going to use a second-order Taylor expansion this time (a quadratic function). The F is a fisher information matrix,  which is the expected value of the outer product of our gradients.   

<p align="center">
<img src="/images/245.png"><br/>
</p>
#### 5. Now we can replace the KL divergence constraint with the much convenient quadratic constraint (the Euclidean distance is also quadratic). We can think that the F rotated and scaled everything, so we get the ellipse in the picture. We need the F inverse to do the update. The gradient is called the natural gradient because this gradient will change the distribution by the same amount regardless how we parameterize it.    

<p align="center">
<img src="/images/246.png"><br/>
</p>

#### 6. Is this even a problem in practice? We have a toy problem as below. The distribution is represented by the mean (k) and variance (σ). The policy optimization consists of figuring out the best k and σ to maximize the total expected reward. The blue point can do a Gaussian random walk bias by the k. We can see the vanilla policy gradient doesn't point to the optimum and the natural policy gradient can do it better.

<p align="center">
<img src="/images/247.png"><br/>
</p>

### (12) Practical methods and notes

#### 1. For natural policy gradient, there are a little bit of numerical tricks to compute F inverse times gradient of J(θ). We don't want to compute the full fisher information matrix because the computation cost is too large (each neural network has so many parameters). In practice, there's a trick to calculate F inverse grad J by repeatedly calculating Fisher-vector times grad J using the conjugate gradient algorithm (described in TRPO paper appendix). 

#### 2. In natural policy gradient, we choose the α ourselves, but in TRPO, we choose the ε ourselves, and we calculate the α using the following rule, which ensures that our KL divergence according to the Taylor expansion changes a fixed amount every step. We can think of this as an adaptive step size adjustment kind of a fisher version of Adam specialized to natural gradient. 

#### 3. We can use the importance sampled objective directly. We can try the methods like the lagrangian as before and the practical method PPO.    

<p align="center">
<img src="/images/248.png"><br/>
</p>

### (13) Review

<p align="center">
<img src="/images/249.png"><br/>
</p>