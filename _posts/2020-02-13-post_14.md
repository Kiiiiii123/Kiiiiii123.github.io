### Hung-yi Lee GAN Lecture 1: Introduction

### Outline

- #### Basic idea of GAN


- #### GAN as structured learning


- #### Can Generator learn by itself?


- #### Can Discriminator generate?


- #### A little bit theory


### Generation process

#### With GAN, what we want our machine to do is to generate something. So we need to train a generator in the generation process. When we feed different vectors sampled from some distribution (like Gaussian) into the generator, we can get different images (sentences). Conditional generation can have a practical effect. 

<p align="center">
<img src="/images/176.png"><br/>
</p>

### Basic idea of GAN

- #### Generator is a neural network or a function. Each dimension of input vector represents some characteristics of image (or sentence). 

<p align="center">
<img src="/images/177.png"><br/>
</p>

- #### Discriminator is also a neural network (or a function), whose input is an image (or sentence) and output is a scalar. The scalar represents the quality of the generated image or sentence. Larger value means real, smaller value means fake. 

<p align="center">
<img src="/images/178.png"><br/>
</p>

- #### The relationship between generator and discriminator is just like hunter and prey.

<p align="center">
<img src="/images/179.png"><br/>
</p>

- #### We need a database of real images for the training procedure. Both of the generator and discriminator evolve to beat each other.

<p align="center">
<img src="/images/180.png"><br/>
</p>

- #### We can also use another metaphor to explain. But why can't generator learn bu itself? Is the discriminator indispensable? Maybe generator can learn from samples or maybe the discriminator can generate data by itself?  

<p align="center">
<img src="/images/181.png"><br/>
</p>

### Generator VS Discriminator

#### The generator and discriminator are enemies and friends.

<p align="center">
<img src="/images/182.png"><br/>
</p>

### Algorithm

- #### Step one: fix generator G and update discriminator D. Discriminator learn to assign high scores to real objects and low scores to generated objects.

<p align="center">
<img src="/images/183.png"><br/>
</p>

- #### Step two: fix discriminator D and update generator G. Generator learns to "fool" the discriminator.  In practice, we combine generator and discriminator to get a large neural network and fix the parameters of last several hidden layers. 

<p align="center">
<img src="/images/184.png"><br/>
</p>

- #### Pesudocode of the algorithm

<p align="center">
<img src="/images/185.png"><br/>
</p>

### Anime Face Generation

<p align="center">
<img src="/images/186.png"><br/>
</p>

### GAN as structured learning

#### Let's talk about the principle behind GAN.

- #### Structured learning

<p align="center">
<img src="/images/187.png"><br/>
</p>

- #### Output sequence


<p align="center">
<img src="/images/188.png"><br/>
</p>

- #### Output matrix

<p align="center">
<img src="/images/189.png"><br/>
</p>

- #### Why structured learning challenging?

##### Structured learning problem can be  seen as an extreme case of one-shot/zero-shot learning problem. Machine has to create new stuff during testing, thus needs more intelligence.

<p align="center">
<img src="/images/190.png"><br/>
</p>

##### Machine has to learn to do planning (or to be diligent) to output complex stuff, which consists of many components.

<p align="center">
<img src="/images/191.png"><br/>
</p>

- #### Structured learning approach

##### Combine generator (bottom up) and discriminator (top down) to get GAN.

<p align="center">
<img src="/images/192.png"><br/>
</p>

### Can generator learn by itself?

#### Absolutely. With a real image database, we can input vectors and train the network to output fake images. If the generated images are similar, then the corresponding vectors should be related to each other.

<p align="center">
<img src="/images/193.png"><br/>
</p>

#### We can learn an encoder, whose input can be an image and output can be the feature vector of the image.

<p align="center">
<img src="/images/194.png"><br/>
</p>

#### Encoder can't learn by itself, so we combine it with a decoder, which can convert the vector to image, then we'll get the auto-encoder. We input an image to auto-encoder and output a similar image. We want the two images to be as close as possible.

<p align="center">
<img src="/images/195.png"><br/>
</p>

#### The decoder is just like the a generator. 

<p align="center">
<img src="/images/196.png"><br/>
</p>

#### Let's learn a generator which can output digital images. 

<p align="center">
<img src="/images/197.png"><br/>
</p>

<p align="center">
<img src="/images/198.png"><br/>
</p>

#### There's a problem when we use auto-encoder:  data on the training set are limited. The trained generator maybe can't output correct image when the input is the average of vector a and vector b . 

<p align="center">
<img src="/images/199.png"><br/>
</p>

#### How to solve the problem? Variational Auto-encoder (VAE). The encoder in VAE not only outputs the code, it also outputs the variance of the code and noise from a normal distribution. We'll input code + variance * noise into the decoder in VAE and then the generator will be more stable.  

<p align="center">
<img src="/images/200.png"><br/>
</p>

#### What do we miss when training auto-encoder? We want the two images to be as close as possible. It'll be fine if the generator can truly copy the target image. But what if the generator makes some mistakes? Some mistakes are serious, while some are fine. The generator doesn't have the capacity to output the exact same image as the real one, so it needs to make some compromises. Where to make the compromises becomes very important.  

<p align="center">
<img src="/images/201.png"><br/>
</p>

#### Take the following as an example, people and machine have very different views. We can tolerate some big mistake, but can't tolerate some little mistake.

<p align="center">
<img src="/images/202.png"><br/>
</p>

#### The relation between components in structured learning are critical. Each neural in output layer corresponds to a pixel. Although highly correlated, they can't influence each other. So we need deep structure (more hidden layers) to catch the relation between components. In practical, we need larger neural network to get the output that is close to what GAN can output.

<p align="center">
<img src="/images/203.png"><br/>
</p>

### Can discriminator generate?

#### Discriminator: input object x and output a scalar.

<p align="center">
<img src="/images/204.png"><br/>
</p>

#### It's easier to catch the relation between the components by top-down evaluation.

<p align="center">
<img src="/images/205.png"><br/>
</p>

#### How to use discriminator to generate something (suppose we already have a good discriminator)? Enumerate all possible x.

<p align="center">
<img src="/images/206.png"><br/>
</p>

#### How to train a good discriminator? We just have some positive samples (real images), so we need some negative samples.

<p align="center">
<img src="/images/207.png"><br/>
</p>

#### Negative samples are critical. We should choose them seriously. But how to generate realistic fake examples?

<p align="center">
<img src="/images/208.png"><br/>
</p>

#### We need an iterative method. The general algorithm is as the following.

<p align="center">
<img src="/images/209.png"><br/>
</p>

#### In practice, we can't decrease all the x other than real examples. The training process of the algorithm is just like the following.

<p align="center">
<img src="/images/210.png"><br/>
</p>

<p align="center">
<img src="/images/211.png"><br/>
</p>

#### There are many work that use the discriminator to generate something.

<p align="center">
<img src="/images/212.png"><br/>
</p>

#### Generator VS Discriminator: The "argmax" problem is hard to solve.

<p align="center">
<img src="/images/213.png"><br/>
</p>

#### Generator + Discriminator: We just use generator to generate negative examples and avoid the "argmax" problem.

<p align="center">
<img src="/images/214.png"><br/>
</p>

#### Benefit of GAN

<p align="center">
<img src="/images/215.png"><br/>
</p>
