### Paper1: Trust Region Policy Optimization (TRPO)

### Abstract

- #### A method for optimizing control policies, with guaranteed monotonic improvement 


- #### Making several approximations to the theoretically-justified scheme


- #### Effective for optimizing large nonlinear policies, such as neural networks


- #### Robust performance on a wide variety of tasks


- #### Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement


### 1 Introduction

#### Most algorithms for policy optimization can be classified into three categories: policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy; policy gradient methods, which use an estimator of the gradient of the expected cost obtained from sample trajectories (have a close connection to policy iteration); derivative-free optimization methods. 

#### In this article, we first prove that minimizing a certain surrogate loss function guarantees policy improvement with non-trivial step sizes. Then we make a series of approximations to the theoretically-justified algorithm, yielding a practical algorithm.

#### We describe two variants of this algorithm: first, the single-path method, which can be applied in the model-free setting; second, the vine-method, which requires the system to be stored to particle states, which is typically only possible in simulation. 

### 2 Preliminaries

#### Let η(π) denote its expected discounted rewards:

<p align="center">
<img src="/images/250.png"><br/>
</p>

#### Standard definitions of the state-action value function Qπ, the value function Vπ, and the advantage function Aπ:

<p align="center">
<img src="/images/251.png"><br/>
</p>

#### The following useful identity expresses the expected cost of another policy π bar, in terms of the advantage over π:

<p align="center">
<img src="/images/252.png"><br/>
</p>

#### Proof:

<p align="center">
<img src="/images/253.png"><br/>
</p>

#### We want the second term on the right-hand side of Equation(1) to be positive to update the policy, but the equation now can't give us much information, so we rearrange the Equation(1) to sum over states instead of time steps.

<p align="center">
<img src="/images/254.png"><br/>
</p>

#### In Equation(2), we let ρπ be the (unnormalized) discounted visitation frequencies:

<p align="center">
<img src="/images/255.png"><br/>
</p>

#### The Equation(2) implies that any policy update from π to π bar that has a nonnegative expected advantage at every state s. However, in the approximate setting, due to estimation and approximation error, there'll be some states s for which the expected advantage is negative. The complex dependency of ρπ bar(s) on π bar makes Equation(2) difficult to optimize directly. Instead, we introduce the following local approximation to η:

<p align="center">
<img src="/images/256.png"><br/>
</p>

#### The Lπ uses the visitation sequence ρπ rather ρπ bar, ignoring changes in state visitation density due to changes in policy. However, if we have a parameterized policy πθ, where πθ(a|s) is a differentiable function of the parameter vector θ, then Lπ matches η to first order. That is, for any parameter value θ0,

<p align="center">
<img src="/images/257.png"><br/>
</p>

#### Equation(4) implies that a sufficiently small step from πθ0 to π bar that improves Lπθold will also improve η, but doesn't give us any guidance on how big of a step to take.  

<p align="center">
<img src="/images/258.png"><br/>
</p>

#### To address this, Kakade & Langford (2002) proposed a policy updating scheme called conservative （保守的）policy iteration, for which we could provide explicit lower bounds on the improvement of η. 

#### The new policy πnew was defined to be the following mixture:

<p align="center">
<img src="/images/259.png"><br/>
</p>

#### Kakade and Langford derived the following lower bound:

<p align="center">
<img src="/images/260.png"><br/>
</p>

#### However, that so far this bound only applies to mixture policies generated by Equation (5). This policy class is unwieldy and restrictive in practice, and it is desirable for a practical policy update scheme to be applicable to all general stochastic policy classes.

### 3 Monotonic Improvement Guarantee for General Stochastic Policies

#### Equation(6), which applies to conservative policy iteration, implies that a policy update that improves the right-hand side is guaranteed to improve the true performance η. Our principal theoretical result is that the policy improvement bound in Equation (6) can be extended to general stochastic policies, rather than just mixture polices, by replacing α with a distance measure between π and π bar.  Since mixture policies are rarely used in practice, this result is crucial for extending the improvement guarantee to practical problems. The particular distance measure we use is the total variation divergence, which is defined by:

<p align="center">
<img src="/images/261.png"><br/>
</p>

#### for discrete probability distribution p,q. Then we define:

<p align="center">
<img src="/images/262.png"><br/>
</p>

