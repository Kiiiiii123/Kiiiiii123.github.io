### Paper: Trust Region Policy Optimization (TRPO)

### Abstract

- #### A method for optimizing control policies, with guaranteed monotonic improvement 


- #### Making several approximations to the theoretically-justified scheme


- #### Effective for optimizing large nonlinear policies, such as neural networks


- #### Robust performance on a wide variety of tasks


- #### Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement


### 1 Introduction

#### Most algorithms for policy optimization can be classified into three categories: policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy; policy gradient methods, which use an estimator of the gradient of the expected cost obtained from sample trajectories (have a close connection to policy iteration); derivative-free optimization methods. 

#### In this article, we first prove that minimizing a certain surrogate loss function guarantees policy improvement with non-trivial step sizes. Then we make a series of approximations to the theoretically-justified algorithm, yielding a practical algorithm.

#### We describe two variants of this algorithm: first, the single-path method, which can be applied in the model-free setting; second, the vine-method, which requires the system to be stored to particle states, which is typically only possible in simulation. 

### 2 Preliminaries

#### Let η(π) denote its expected discounted rewards:

<p align="center">
<img src="/images/250.png"><br/>
</p>

#### Standard definitions of the state-action value function Qπ, the value function Vπ, and the advantage function Aπ:

<p align="center">
<img src="/images/251.png"><br/>
</p>

#### The following useful identity expresses the expected cost of another policy π bar, in terms of the advantage over π:

<p align="center">
<img src="/images/252.png"><br/>
</p>

#### Proof:

<p align="center">
<img src="/images/253.png"><br/>
</p>