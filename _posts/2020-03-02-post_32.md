### Paper 15: Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (SAC)

### Abstract

- #### Model-free DRL algorithms typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous（细致的）hyperparameter tuning.


- #### We propose soft actor-critic, an off-policy actor-critic DRL algorithm based on the maximum entropy (act as randomly as possible) reinforcement learning framework. 

- #### Our method combines off-policy updates with a stable stochastic actor-critic formulation.


### 1 Introduction

#### Widespread adoption of model-free DRL algorithms in real-world domains has been hampered by two major challenges.

- #### First, these methods are `notoriously expensive in terms of their sample complexity`. Even relatively simple tasks can require millions of steps of data collection, and complex behaviors with high dimensional observations might need substantially more.

- #### Second, these methods are often `brittle with respect to their hyperparameters`: learning rates, exploration constants, and other settings must be set carefully for different problem settings to achieve good results. 

#### Both of these challenges severely limit the applicability of model-free DRL to real-world tasks.

#### One cause for the poor sample efficiency of DRL methods is on-policy learning. Off-policy algorithms aim to reuse past experience. Unfortunately, the combination of off-policy learning and high-dimensional, nonlinear function approximation with neural networks presents a major challenge for stability and convergence.

#### We explore how to design an efficient and stable model-free DRL algorithm for continuous state and action spaces. To that end, we draw on the `maximum entropy framework`, which augments the standard maximum reward reinforcement learning objective with an entropy maximization term.

#### In this paper, we demonstrate that we can devise an `off-policy maximum entropy actor-critic algorithm`, which we call soft actor-critic (SAC), which provides for `both sample-efficient learning and stability`. We present a `convergence proof for policy iteration in the maximum entropy framework`, and then introduce a new algorithm based on an approximation to this procedure that can be practically implemented with deep neural networks, which we call soft actor-critic. 

### 3 Preliminaries

#### We first introduce notation and summarize the standard and `maximum entropy reinforcement learning frameworks`.

#### 3.1 Notation

#### 3.2 Maximum Entropy Reinforcement Learning

#### Standard RL maximizes the expected sum of rewards

<p align="center">
<img src="/images/473.png"><br/>
</p>
#### We'll consider `a more general maximum entropy objective`, which favors stochastic policies by augmenting the objective with the expected entropy of the policy over ρπ(st):

<p align="center">
<img src="/images/474.png"><br/>
</p>
#### The temperature parameter `α determines the relative importance of the entropy term against the reward`, and thus `controls the stochasticity of the optimal policy`. The maximum entropy objective differs from the standard maximum expected reward objective used in conventional（传统的）reinforcement learning, though the conventional objective can be recovered in the limit as α → 0. For the rest of this paper, we will omit（省略）writing the temperature explicitly, as it can always be `subsumed（归入）into the reward by scaling it by α−1`.

#### This objective has a number of `conceptual and practical advantages`.

- #### First, the policy is incentivized（激励）to explore more widely, while giving up on clearly unpromising avenues.

- #### Second, the policy can capture multiple modes of near-optimal behavior. In problem settings where multiple actions seem equally attractive, the policy will commit equal probability mass to those actions.

- #### Lastly, prior work has observed improved exploration with this objective, and in our experiments, we observe that it considerably improves learning speed over state-of-art methods that optimize the conventional RL objective function.

#### We can `extend the objective to infinite horizon problems` by introducing a discount factor γ to ensure that the sum of expected rewards and entropies is finite.

#### We will discuss how we can `devise a soft actor-critic algorithm through a policy iteration formulation`, where we instead evaluate the Q-function of the current policy and update the policy through an off-policy gradient update. Though such algorithms have previously been proposed for conventional reinforcement learning, our method is, to our knowledge, `the first off-policy actor-critic method in the maximum entropy reinforcement learning framework`.

### 4 From Soft Policy Iteration to Soft Actor-Critic

#### Our off-policy soft actor-critic algorithm can `be derived starting from a maximum entropy variant of the policy iteration method`. We will first present this derivation, verify that the `corresponding algorithm converges to the optimal policy from its density class`, and then present `a practical deep reinforcement learning algorithm based on this theory`.

#### 4.1 Derivation of Soft Policy Iteration 

#### We will `begin by deriving soft policy iteration`, a general algorithm for learning optimal maximum entropy policies that alternates between policy evaluation and policy improvement in the maximum entropy framework. Our derivation is `based on a tabular setting`, to enable theoretical analysis and convergence guarantees, and we `extend this method into the general continuous setting` in the next section. We will show that `soft policy iteration converges to the optimal policy within a set of policies` which might correspond, for instance, to a set of parameterized densities.

#### In the `policy evaluation step of soft policy iteration`, we wish to compute the value of a policy π according to the maximum entropy objective in Equation 1. For a fixed policy, the soft Q-value can be computed iteratively, starting from any function Q : S ×A → R and `repeatedly applying a modified Bellman backup operator Tπ` given by  

<p align="center">
<img src="/images/475.png"><br/>
</p>

#### where

<p align="center">
<img src="/images/476.png"><br/>
</p>

#### is the `soft state value function`. We can obtain the soft value function for any policy π by repeatedly applying Tπ as formalized below.

<p align="center">
<img src="/images/477.png"><br/>
</p>

#### In the `policy improvement step`, we `update the policy towards the exponential of the new Q-function`. This particular choice of update can be guaranteed to result in an improved policy in terms of its soft value. Since in practice we prefer policies that are tractable, we will `additionally restrict the policy to some set of policies Π`, which can correspond, for example, to a parameterized family of distributions such as Gaussians. To account for the constraint that π ∈ Π, we `project the improved policy into the desired set of policies`. While in principle we could choose any projection, it will turn out to be convenient to use the `information projection defined in terms of the KL divergence`. In the other words, in the policy improvement step, for each state, we update the policy according to

<p align="center">
<img src="/images/478.png"><br/>
</p>

#### The partition function Z normalizes the distribution, and while it is intractable in general, it does not contribute to the gradient with respect to the new policy and can thus be ignored, as noted in the next section. For this projection, we can show that `the new, projected policy has a higher value than the old policy with respect to the objective` in Equation 1. We formalize this result in Lemma 2.

<p align="center">
<img src="/images/479.png"><br/>
</p>

#### The full soft policy iteration algorithm alternates between the soft policy evaluation and the soft policy improvement steps, and it will provably converge to the optimal maximum entropy policy among the policies in Π (Theorem 1). Although this algorithm will provably 
