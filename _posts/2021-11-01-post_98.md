### Tutorial 3: RLlib (3) — Scaling Multi-Agent Reinforcement Learning

### Why multi-agent reinforcement learning?

#### We've observed that in applied RL settings, the question of whether it makes sense to use multi-agent algorithms often comes up. Compared to training a single policy that issues all actions in the environment, multi-agent approaches can offer:

- #### A more natural decomposition of the problem. For example, suppose one wants to train policies for [cellular antenna tilt control](https://ieeexplore.ieee.org/document/1032000)（蜂窝天线倾斜控制）in urban environments. Instead of training a single "super-agent" that controls all the cellular antennas in a city, it is more natural to model each antenna as a separate agent in the environment because only neighboring antennas and user observations are relevant to each site. In general, we may wish to have each agent's action and/or observation space restricted to only model the components that it can affect and those that affect it.

- #### Potential for more scalable learning. First, decomposing the actions and observations of a single monolithic（巨大的）agent into multiple simpler agents not only reduces the dimensionality of agent inputs and outputs, but also effectively increases the amount of training data generated per step of the environment. 

  #### Second, partitioning the action and observation spaces per agent can play a similar role to the approach of imposing temporal abstractions, which has been successful in increasing learning efficiency in the single-agent setting. Relatedly, some of these hierarchical approaches can be implemented explicitly as multi-agent systems.

  #### Finally, good decompositions can also lead to the learning of policies that are more transferable across different variations of an environment, i.e., in contrast to a single super-agent that may over-fit to a particular environment.

<p align="center">
<img src="/images/939.png"><br/>
Figure 1: Single-agent approaches (a) and (b) in comparison with multi-agent RL (c).
</p>

#### Some examples of multi-agent applications include:

- #### [Traffic congestion reduction](https://flow-project.github.io/)（减少交通拥堵）: It turns out that by intelligently controlling the speed of a few autonomous vehicles we can drastically increase the traffic flow. Multi-agent can be a requirement here, since in [mixed-autonomy](https://flow-project.github.io/index.html) settings, it is unrealistic to model traffic lights and vehicles as a single agent, which would involve the synchronization of observations and actions across all agents in a wide area.


<p align="center">
<img src="/images/940.gif"><br/>
Flow simulation without AVs vs With AV agents (red vehicles)
</p>

- #### [Antenna tilt control](https://www.kt.tu-darmstadt.de/media/kt/publikationen_1/10/17/WPC_16_Dandanov.pdf)（天线倾斜控制）: The joint configuration of cellular base stations（蜂窝基站）can be optimized according to the distribution of usage and topology of the local environment. Each base station can be modeled as one of multiple agents covering a city.

  <p align="center">
  <img src="/images/941.png"><br/>
  </p>
  
- #### [OpenAI Five](https://blog.openai.com/openai-five/): Dota 2 AI agents are trained to coordinate with each other to compete against humans. Each of the five AI players is implemented as a separate [neural network](https://s3-us-west-2.amazonaws.com/openai-assets/dota_benchmark_results/network_diagram_08_06_2018.pdf) policy and trained together with [large-scale PPO](https://blog.openai.com/openai-five/).

  <p align="center">
  <img src="/images/942.png"><br/>
  </p>

### Introducing multi-agent support in RLlib

#### In this tutorial, we introduce general purpose support for multi-agent RL in RLlib, including compatibility with most of RLlib's distributed algorithms: A2C / A3C, PPO, IMPALA, DQN, DDPG, and Ape-X. In the remainder of this tutorial we discuss the challenges of multi-agent RL, show how to train multi-agent policies with these existing algorithms, and also how to implement specialized algorithms that can deal with the non-stationarity and increased variance of multi-agent environments.

#### There are currently few libraries for multi-agent RL, which increases the initial overhead of experimenting with multi-agent approaches. RLlib's goal is to reduce this friction and make it easy to go from single-agent to multi-agent RL in both research and application.

### Why supporting multi-agent is hard

#### Building software for a rapidly developing field such as RL is challenging, multi-agent especially so. This is in part due to the breadth of techniques used to deal with the core issues that arise in multi-agent learning.

#### Consider one such issue: environment non-stationarity. In the below figure, the red agent's goal is to learn how to regulate（调节）the speed of the traffic. The blue agents are also learning, but only to greedily minimize their own travel time. The red agent may initially achieve its goal by simply driving at the desired speed. However, in a multi-agent environment, the other agents will learn over time to meet their own goals -- in this case, bypassing the red agent. This is problematic since from a single-agent view (i.e., that of the red agent), the blue agents are "part of the environment". The fact that environment dynamics are changing from the perspective of a single agent violates（违背）the Markov assumptions required for the convergence of Q-learning algorithms such as DQN.

<p align="center">
<img src="/images/943.png"><br/>
Figure 2: Non-stationarity of environment: Initially (a), the red agent learns to regulate the speed of the traffic by slowing down. However, over time the blue agents learn to bypass the red agent (b), rendering the previous experiences of the red agent invalid.
</p>

#### A number of algorithms have been proposed that help address this issue, e.g., [LOLA](https://arxiv.org/abs/1709.04326), [RIAL](https://arxiv.org/pdf/1605.06676.pdf), and [Q-MIX](https://arxiv.org/abs/1803.11485). At a high level, these algorithms take into account the actions of the other agents during RL training, usually by being partially centralized for training, but decentralized during execution. Implementation wise, this means that the policy networks may have dependencies on each other, i.e., through a mixing network in Q-MIX:

<p align="center">
<img src="/images/944.png"><br/>
Figure 3: The Q-mix network architecture, from Q-MIX. Individual Q-estimates are aggregated by a monotonic mixing network（单调混合网络）for efficiency of final action computation.
</p>

#### Similarly, policy-gradient algorithms like A3C and PPO may struggle in multi-agent settings, as the credit assignment problem becomes increasingly harder with more agents. Consider a traffic gridlock between many autonomous agents. It is easy to see that the reward given to an agent -- here reflecting that traffic speed has dropped to zero -- will have less and less correlation with the agent's actions as the number of agents increases:

<p align="center">
<img src="/images/945.png"><br/>
Figure 4: High variance of advantage estimates: In this traffic gridlock situation, it is unclear which agents' actions contributed most to the problem -- and when the gridlock is resolved, from any global reward it will be unclear which agents get credit.
</p>

#### One class of approaches here is to model the effect of other agents on the environment with centralized value functions (the "Q" boxes in Figure 5) function, as done in [MA-DDPG](https://arxiv.org/abs/1706.02275). Intuitively, the variability of individual agent advantage estimates can be greatly reduced by taking into account the actions of other agents:

<p align="center">
<img src="/images/946.png"><br/>
Figure 5: The MA-DDPG architecture. Policies run using only local information at execution time, but may take advantage of global information at training time.
</p>

#### So far we've seen two different challenges and approaches for tackling multi-agent RL. That said, in many settings training multi-agent policies using single-agent RL algorithms can yield surprisingly strong results. For example, OpenAI Five has been successful using only a combination of large-scale PPO and a specialized network model. The only considerations for multi-agent are the annealing（退火）of a "team spirit" hyperparameter that influences the level of shared reward, and a shared "max-pool across Players" operation in the model to share observational information.

### Multi-agent training in RLlib

#### So how can we handle both these specialized algorithms and also the standard single-agent RL algorithms in multi-agent settings? Fortunately RLlib's design makes this fairly simple. The relevant principles to multi-agent are as follows:

- #### Policies are represented as objects: All gradient-based algorithms in RLlib declare a policy graph object, which includes a policy model πθ(ot), a trajectory postprocessing function postθ(traj), and finally a policy loss L(θ; X). This policy graph object specifies enough for the distributed framework to execute environment rollouts (by querying πθ), collate experiences (by applying postθ), and finally to improve the policy (by descending L(θ; X)).

- #### Policy objects are black boxes: To support multi-agent, RLlib just needs to manage the creation and execution of multiple policy graphs per environment, and add together the losses during policy optimization. Policy graph objects are treated largely as black boxes by RLlib, which means that they can be implemented in any framework including TensorFlow and PyTorch. Moreover, policy graphs can internally share variables and layers to implement specialized algorithms such as Q-MIX and MA-DDPG, without special framework support.

#### To make the application of these principles concrete, In the next few sections we walk through code examples of using RLlib's multi-agent APIs to execute multi-agent training at scale.

### Multi-agent environment model

#### The engineers are not aware of a standard multi-agent environment interface, so they wrote their own as a straightforward extension of the gym interface. In a multi-agent environment, there can be multiple acting entities per step. As a motivating example, consider a traffic control scenario (Figure 6) where multiple controllable entities (e.g., traffic lights, autonomous vehicles) work together to reduce highway congestion.

#### In this scenario,

- #### Each of these agents can act at different time-scales (i.e., act asynchronously).

- #### Agents can come and go from the environment as time progresses.

<p align="center">
<img src="/images/947.png"><br/>
Figure 6. RLlib multi-agent environments can model multiple independent agents that come and go over time. Different policies can be assigned to agents as they appear.
</p>

#### This is formalized in the [MultiAgentEnv](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) interface, which can returns observations and rewards from multiple ready agents per step:

```python
# Example: using a multi-agent env
> env = MultiAgentTrafficEnv(num_cars=20, num_traffic_lights=5)

# Observations are a dict mapping agent names to their obs. Only those
# agents' names that require actions in the next call to `step()` will
# be present in the returned observation dict.
> print(env.reset())
{
    "car_1": [[...]],
    "car_2": [[...]],
    "traffic_light_1": [[...]],
}

# In the following call to `step`, actions should be provided for each
# agent that returned an observation before:
> new_obs, rewards, dones, infos = env.step(actions={"car_1": ..., "car_2": ..., "traffic_light_1": ...})

# Similarly, new_obs, rewards, dones, etc. also become dicts
> print(rewards)
{"car_1": 3, "car_2": -1, "traffic_light_1": 0}

# Individual agents can early exit; The entire episode is done when "__all__" = True
> print(dones)
{"car_2": True, "__all__": False}
```

#### Any Discrete, Box, Dict, or Tuple [observation space](https://github.com/openai/gym/tree/master/gym/spaces) from OpenAI gym can be used for these individual agents, allowing for multiple sensory inputs per agent (including communication between agents, it desired).

### Multiple levels of API support

#### At a high level, RLlib models agents and policies as objects that may be bound to each other for the duration of an episode (Figure 7). Users can leverage this abstraction to varying degrees, from just using a single-agent shared policy, to multiple policies, to fully customized policy optimization:

<p align="center">
<img src="/images/948.png"><br/>
Figure 7: The multi-agent execution model in RLlib compared with single-agent execution.
</p>

1. #### Level 1: Multiple agents, shared policy

   #### If all "agents" in the env are homogeneous (e.g., multiple independent cars in a traffic simulation), then it is possible to use existing single-agent algorithms for training. Since there is still only a single policy being trained, RLlib only needs to internally aggregate the experiences of the different agents prior to policy optimization. The change is minimal on the user side:

   #### from single-agent:

   ```python
   register_env("your_env", lambda c: YourEnv(...))
   trainer = PPOTrainer(env="your_env")
   while True:
      print(trainer.train())  # distributed training step
   ```

   #### to multi-agent:

   ```python
   register_env("your_multi_env", lambda c: YourMultiEnv(...))
   trainer =  PPOTrainer(env="your_multi_env")
   while True:
      print(trainer.train())  # distributed training step
   ```

2. #### Level 2: Multiple agents, multiple policy

   #### To handle multiple policies, this requires the definition of which agent(s) are handled by each policy. We handle this in RLlib via a policy mapping function, which assigns agents in the env to a particular policy when the agent first enters the environment. In the following examples we consider a hierarchical control setting where supervisor agents assign work to teams of worker agents they oversee. The desired configuration is to have a single supervisor policy and an ensemble of two worker policies:

   ```python
   def policy_mapper(agent_id):
      if agent_id.startswith("supervisor_"):
          return "supervisor_policy"
      else:
          return random.choice(["worker_p1", "worker_p2"])
   ```

   #### In the example, we always bind supervisor agents to the single supervisor policy, and randomly divide other agents between an ensemble of two different worker policies. These assignments are done when the agent first enters the episode, and persist for the duration of the episode. Finally, we need to define the policy configurations, now that there is more than one. This is done as part of the top-level agent configuration:

   ```python
   trainer = PPOTrainer(env="control_env", config={
      "multiagent": {
          "policy_mapping_fn": policy_mapper,
          "policies": {
              "supervisor_policy":
                   (PPOTorchPolicy, sup_obs_space, sup_act_space, sup_conf),
              "worker_p1": (
                  (PPOTorchPolicy, work_obs_s, work_act_s, work_p1_conf),
              "worker_p2":
                   (PPOTorchPolicy, work_obs_s, work_act_s, work_p2_conf),
          },
          "policies_to_train": [
                "supervisor_policy", "worker_p1", "worker_p2"],
      },
   })
   while True:
      print(trainer.train())  # distributed training step
   ```

   #### This would generate a configuration similar to that shown in Figure 2. You can pass in a custom Policy class for each policy, as well as different policy config dicts. This allows for any of RLlib's support for customization (e.g., [custom models and preprocessors](https://ray.readthedocs.io/en/latest/rllib-models.html#custom-models)) to be used per policy, as well as wholesale（大批的）definition of a new class of policy.

   #### Advanced examples:

   - #### [Sharing layers across policies](https://ray.readthedocs.io/en/latest/rllib-env.html#variable-sharing-between-policies)

   - #### [Implementing a centralized critic](https://ray.readthedocs.io/en/latest/rllib-env.html#implementing-a-centralized-critic)

3. #### Level 3: Custom training strategies

   #### For advanced applications or research use cases, it is inevitable to run into the limitations of any framework.

   #### For example, let's suppose multiple training methods are desired: some agents will learn with PPO, and others with DQN. This can be done in a way by swapping weights between two different trainers, but this approach won't scale with even more types of algorithms thrown in, or if e.g., you want to use experiences to train a model of the environment at the same time.
   
   #### In these cases we can fall back to RLlib's underlying system [Ray](https://bair.berkeley.edu/blog/2018/01/09/ray/) to distribute computations as needed. Ray provides two simple parallel primitives:
   
   - #### [Tasks](https://ray.readthedocs.io/en/latest/tutorial.html), which are Python functions executed asynchronously via func.remote()
   
   - #### [Actors](https://ray.readthedocs.io/en/latest/actors.html), which are Python classes created in the cluster via class.remote(). Actor methods can be called via actor.method.remote().
   
   #### RLlib builds on top of Ray tasks and actors to provide a toolkit for distributed RL training. This includes:
   
   - #### [Policies](https://docs.ray.io/en/latest/rllib-concepts.html#policies) (as seen in previous examples)
   
   - #### [Policy evaluation](https://docs.ray.io/en/latest/rllib-concepts.html#policy-evaluation)
   
     #### Given an environment and policy, policy evaluation produces [batches](https://github.com/ray-project/ray/blob/master/rllib/policy/sample_batch.py) of experiences. This is your classic “environment interaction loop”. Efficient policy evaluation can be burdensome（繁重的）to get right, especially when leveraging vectorization, RNNs, or when operating in a multi-agent environment. RLlib provides a [RolloutWorker](https://github.com/ray-project/ray/blob/master/rllib/evaluation/rollout_worker.py) class that manages all of this, and this class is used in most RLlib algorithms.
   
     #### You can use rollout workers standalone to produce batches of experiences. This can be done by calling *worker.sample()* on a worker instance, or *worker.sample.remote()* in parallel on worker instances created as Ray actors (see [WorkerSet](https://github.com/ray-project/ray/blob/master/rllib/evaluation/worker_set.py)).
   
     #### Here is an example of creating a set of rollout workers and using them gather experiences in parallel. The trajectories are concatenated, the policy learns on the trajectory batch, and then we broadcast the policy weights to the workers for the next round of rollouts:
   
     ```python
     # Setup policy and rollout workers
     env = gym.make("CartPole-v0")
     policy = CustomPolicy(env.observation_space, env.action_space, {})
     workers = WorkerSet(
         policy_class=CustomPolicy,
         env_creator=lambda c: gym.make("CartPole-v0"),
         num_workers=10)
     
     while True:
         # Gather a batch of samples
         T1 = SampleBatch.concat_samples(
             ray.get([w.sample.remote() for w in workers.remote_workers()]))
     
         # Improve the policy using the T1 batch
         policy.learn_on_batch(T1)
     
         # Broadcast weights to the policy evaluation workers
         weights = ray.put({"default_policy": policy.get_weights()})
         for w in workers.remote_workers():
             w.set_weights.remote(weights)
     ```
   
   - #### [Execution Plans](https://docs.ray.io/en/latest/rllib-concepts.html#execution-plans)
   
     #### Execution plans let you easily express the execution of an RL algorithm as a sequence of steps that occur either sequentially in the learner, or in parallel across many actors. Under the hood, RLlib translates these plans into *ray.get()* and *ray.wait()* operations over Ray actors, so you easily write high-performance algorithms without needing to manage individual low-level Ray actor calls.
   
     #### Execution plans represent the dataflow of the RL training job. For example, the A2C algorithm can be thought of a sequence of repeating steps, or dataflow, of:
   
     1. #### ParallelRollouts: Generate experiences from many envs in parallel using rollout workers.
   
     2. #### ConcatBatches: The experiences are concatenated into one batch for training.
   
     3. #### TrainOneStep: Take a gradient step with respect to the policy loss, and update the worker weights.
   
     #### In code, this dataflow can be expressed as the following execution plan, which is a simple function that can be passed to *build_trainer* to define a new algorithm. It takes in a *WorkerSet* and *config*, and returns an iterator over training results:
   
     ```python
     def execution_plan(workers: WorkerSet, config: TrainerConfigDict):
         # type: LocalIterator[SampleBatchType]
         rollouts = ParallelRollouts(workers, mode="bulk_sync")
     
         # type: LocalIterator[(SampleBatchType, List[LearnerStatsDict])]
         train_op = rollouts \
             .combine(ConcatBatches(
                 min_batch_size=config["train_batch_size"])) \
             .for_each(TrainOneStep(workers))
     
         # type: LocalIterator[ResultDict]
         return StandardMetricsReporting(train_op, workers, config)
     ```
   
     #### As you can see, each step returns an iterator over objects (if you’re unfamiliar with distributed iterators, see Ray’s [parallel iterators implementation](https://github.com/ray-project/ray/blob/master/python/ray/util/iter.py)). The reason it is a *LocalIterator* is that, though it is based on a parallel computation, the iterator has been turned into one that can be consumed locally in sequence by the program. A couple other points to note:
   
     - #### The reason the plan returns an iterator over training results, is that *trainer.train()* is pulling results from this iterator to return as the result of the train call.
   
     - #### The rollout workers have been already created ahead of time in the *WorkerSet*, so the execution plan function is only defining a sequence of operations over the results of the rollouts.
   
     #### These iterators represent the infinite stream of data items that can be produced from the dataflow. Each operator (e.g., *ConcatBatches*, *TrainOneStep*), executes an operation over each item and returns a transformed item (e.g., concatenated batches, learner stats from training). Finally, some operators such as *TrainOneStep* have the side-effect of updating the rollout worker weights (that’s why *TrainOneStep* takes the list of worker actors *workers* as an argument).
   
     #### One of the primary motivations behind execution plans, beyond their conciseness（简洁）, is to enable complex multi-agent training workflows to be easily composed. For example, suppose one wanted to, in a multi-agent environment, concurrently train one set of agents with DQN, and another set with PPO. This requires stitching together two entirely different distributed dataflows. Fortunately, as we’ve seen earlier, this is quite simple with the *Concurrently* operator.
   
     #### Check out the [PPO + DQN multi-agent workflow example](https://github.com/ray-project/ray/blob/master/rllib/examples/two_trainer_workflow.py) for more details. One line to pay particular attention to in this example is the use of *LocalIterator.duplicate()* to clone the iterator of experiences into two separate iterators, which are filtered via *SelectExperiences* and then consumed by PPO and DQN sub-dataflows respectively.

#### In summary, RLlib provides several levels of APIs targeted at progressively more requirements for customizability. At the highest levels this provides a simple "out of the box" training process, but you retain the option to piece together your own algorithms and training strategies from the core multi-agent abstractions.

### Performance

#### RLlib is designed to scale to large clusters -- and this applies to multi-agent mode as well -- but we also apply optimizations such as vectorization for single-core efficiency. This allows for effective use of the multi-agent APIs on small machines.

#### To show the importance of these optimizations, in the below graph we plot single-core policy evaluation throughout vs the number of agents in the environment. For this benchmark the observations are small float vectors, and the policies are small 16x16 fully connected networks. We assign each agent to a random policy from a pool of 10 such policy networks. RLlib manages over 70k actions/s/core at 10000 agents per environment (the bottleneck becomes Python overhead at this point). When vectorization is turned off, experience collection slows down by 40x:

<p align="center">
<img src="/images/949.png"><br/>
</p>

#### We also evaluate the more challenging case of having many distinct policy networks used in the environment. Here we can still leverage vectorization to fuse multiple TensorFlow calls into one, obtaining more stable per-core performance as the number of distinct policies scales from 1 to 50:

<p align="center">
<img src="/images/950.png"><br/>
</p>
